[
  
  {
    "title"    : "Code without automated tests? Are we serious?",
    "category" : "",
    "tags"     : " Agile, Testing, Refactoring",
    "url"      : "/code-without-automated-tests/",
    "date"     : "October 27, 2021",
    "excerpt"  : "Automated tests are an essential part of every piece of code that we write. The benefits of these tests are so compelling that it does not even\nmake sense to think about writing code without tests or writing code today and adding tests later. Desp...",
  "content"  : "Automated tests are an essential part of every piece of code that we write. The benefits of these tests are so compelling that it does not even\nmake sense to think about writing code without tests or writing code today and adding tests later. Despite the benefits, we still see code without tests,\nwe still see ideas like “writing code today and adding tests when the delivery pressure reduces” floating around.\n\nI don’t have real reasons as to why such ideas would float around but I can speculate.\n\nSpeculation (1)\nAdding tests takes a lot of time\n\nSpeculation (2)\nThere is a lot of delivery pressure today and there is a firm belief that tomorrow will be better and that is when we will go and add tests\n\nWhat I want to do as a part of this article is try and understand how would our world look like with such ideas and does it even make sense to consider them.\nWith that said, I will keep the idea of TDD aside for this article.\n\n\n    What would it really mean to write code without automated tests, to deliver software without automated tests. Why would someone even think of writing code today and adding tests later?\n\n\nDoes it really take time to add tests?\n\nOne of the possible arguments around not adding automated tests could be the “time”.\n\n\n    Argument1: It takes time to add tests and time is really costly. We need to finish our story and there is this delivery pressure.\n\n\nLet’s see how fair is that argument by considering a method leftShift which left shifts the elements of a slice by 1, let’s assume non-empty slice for now.\n\ntype slice struct {\n\telements []int\n}\n\nfunc newSlice(elements []int) *slice {\n\treturn &amp;amp;slice{elements: elements}\n}\n\nfunc (s *slice) leftShift() {\n\tfor index := 0; index &amp;lt; len(s.elements)-1; index++ {\n\t\ts.elements[index] = s.elements[index+1]\n\t}\n\ts.elements[len(s.elements)-1] = 0\n}\n\n\n    \n\n\nWhat this code does is pretty simple -\n\n  drops the element at index 0\n  moves each element to its left\n  puts a zero at the last index\n\n\nLet’s add a unit test for the same.\n\npackage main\n\nimport (\n\t&quot;reflect&quot;\n\t&quot;testing&quot;\n)\n\nfunc TestLeftShiftsANotEmptySliceBy1(t *testing.T) {\n\tslice := newSlice([]int{10, 20, 30, 40, 50})\n\tslice.leftShift()\n\n\texpected := []int{20, 30, 40, 50, 0}\n\n\tif !reflect.DeepEqual(expected, slice.elements) {\n\t\tt.Fatalf(&quot;Expected %v, received %v after performing left shift&quot;,\n\t\t\texpected,\n\t\t\tslice.elements,\n\t\t)\n\t}\n}\n\nAs a part of this test, we perform left shift on a slice and assert the elements against the expected after shift operation. That’s really it.\n\nIt only requires us to understand how to write unit tests.\nHonestly, it doesn’t take a whole lot of time to add tests, be it unit tests, integration tests, contract tests or API tests, once we understand a few things including -\n\n  What do these tests stand for\n    \n      It is essential to understand what is “unit” in a unit test, what is an “integration” test etc\n    \n  \n  What purpose do these tests serve\n    \n      It is essential to answer questions like “why can’t we have all integration tests and zero unit tests”\n    \n  \n  And, how to write these tests\n    \n      It is essential to answer questions like “how do I write tests in X programming language with Y framework”\n    \n  \n\n\nOnce we answer all these questions, it doesn’t take a whole lot of time to add tests.\n\nThere is still a forked argument that I can think of.\n\n\n    Argument2: What we build is not as simple as left shifting a slice, our systems are complex and fancy. We can&#39;t just add one test and be done. Hence, adding so many tests would take time, which of course we don&#39;t have.\n\n\nThe answer to this lies in the argument itself. If left shifting a slice needs an automated test, any fancy and complex system would need them too.\n\nAnd yes, your system is not as simple as left shifting some elements but at the same time, the fancy system is not built in a day. It is built piece by piece\ngradually, so why not add tests for every small piece that gets built.\n\nImportant Side note: If we remove the assumption that our slice is non-empty (ie; elements within the slice struct is non-empty), leftSlice method will fail.\nIn fact, at this point in time, the only way to conclude that a non-empty slice will result in a failure is by walking through the code. Once we have the test\nfor the same, not only does it give us a safety net but also serves as a live documentation which gets updated everytime the behavior of the changes.\n\nCode today and add tests later\n\nOne of the other theories that I have heard is “let’s code today and add tests tomorrow or maybe later”. There could be multiple reasons for this theory\nbut probably, “delivery pressure” and a beautiful belief “code without automated tests is ok” should be the main reasons for this wonderful idea to pop up.\n\nLet’s see what would happen if we write code today and add tests later, forget TDD. Consider our favorite method leftShift and assume -\n\n  no tests are written\n  a week has passed by and now we are adding tests\n\n\nLet’s look at various challenges that would come up -\n\nBoring\n\nI don’t have a better word for this stuff. We need to look at the code and understand what it does. Once that understanding is built, we need to write tests.\nOne might argue, “Why build an understanding”, just write “characterization tests”.\nSure, but how would you figure out corner cases, you need some background to think about corner cases.\n\nThis stuff just seems boring to me, looking at the code, figuring out various cases that too after 7 days, and adding tests.\n\nPossibility of missing corner cases\n\nThere is a good possibility that we will miss corner cases if we decide to add tests later.\n\nLack of motivation\n\nWhat would be the motivation to add tests 7 days later? Lack of coverage in Sonar?\n\nWell, someone might even say this - “the code is already working somewhere (in Dev, QA or even in prod), why bother adding tests now”.\nIt is so easy for such a thought process to set in and once it sets in, tests would only be added for finishing some formality.\n\nDelayed refactoring\n\nIn the absence of tests, refactoring for various parts of code will get delayed. This in turn has a huge drawback, a method which is 50 lines today might\ngrow to 100 lines in 7 days. Not only are the number of code smells going to increase later, but even refactoring might become tricky or may take longer.\n\nWhat are we basing theory of “adding code today and tests later” on? How is tomorrow going to be any different? Is sun going to shine too brightly tomorrow?\nAre we going to stop churning stories tomorrow? Are we going to just add tests and do nothing else?\n\nThe overall idea is just flawed.\n\nI know I could have hurt your emotions, but let’s take a look at some of the benefits of automated tests and see the real gains.\n\nBenefits of automated tests\n\nAutomated tests provide a lot of benefits. I will list a few -\n\nProvide confidence\n\nAutomated tests are like a certification for a working code. I know leftShift is working properly everytime its tests pass.\nIn fact, automated tests act as a mapping between questions and answers. I can ask various questions to unit tests -\n\n  How would leftShift behave if I pass a slice with empty elements\n  How would leftShift behave if I pass a slice with a single element\n  How would leftShift behave if I pass a slice with N elements, where N &amp;gt; 1\n  How would leftShift behave if I pass a slice with N elements containing duplicates, where N &amp;gt; 1\n  How would leftShift behave if I invoke the method with a nil receiver, ie; s of (s *slice) is nil\n\n\nIt is a huge confidence booster :) if all these questions are answered by passing tests.\n\nAct as safety net\n\nAutomated tests are a brilliant safety net, I can go ahead and refactor code without any fear. I know I have tests which would fail loudly if I mess things up, so there is no fear of making mistakes while refactoring.\n\nI think it would be a very courageous move to refactor code without tests. (Honestly, I don’t know if it is a courageous move or a stupid move.)\nBut, if I decide to refactor code without tests, I think I would be plagued by anxiety, there will be a constant banging in the head - what if refactoring breaks the code,\ncan I just stop refactoring here, is it really necessary to refactor etc. And with tests written for the code, there is no case of anxiety or fear.\n\nProvide quick feedback\n\nAutomated tests provide quick feedback on any change that is done in the code.\nAssume (just assume) we are refactoring a long method and we do not have tests. Let’s try and imagine what the world would look like now -\n\n  Extract a piece of code into a new method\n  Run the entire application, send some requests and see if the extraction worked\n  It worked, congratulations\n  Rename the extracted method\n  Run the entire application, send some requests and see if the rename worked\n  It worked, congratulations again\n  Change the number of parameters of the extracted method\n  Run the entire application, send some requests and see if the change in number of parameters worked\n  You really deserve congratulations\n  The method is feature envy, let’s move it to the right class\n\n\n…\n\nIf there were unit tests, we could have run them every time on every change and it would have been way quicker than running the entire application N times.\nThere is a strange part that I don’t seem to understand, I will explain.\nIf the argument for not writing tests or deferring writing tests is “lack of time”, then, where would you get time for running the application N times\nto validate if a change has worked.\n\nAct as documentation\n\nAutomated tests act as documentation for what the code does.\nI don’t need to go through leftShift method to understand its behavior -\n\n  what if it is invoked with a slice containing empty elements\n  what if it is invoked with a slice containing just one element\n\n\nI will just go and look at the tests. In fact, adding tests is one way of creating a trail of understanding for the readers. Readers of the code don’t need to make too much of an effort to find such answers,\nthey can directly look at the tests and find most of the answers.\n\nProvide an opportunity to think from a client’s perspective\n\nAutomated tests give an opportunity to think from a client’s perspective and provide a lot of opportunities to improve the API.\n\nLet’s imagine a struct LinkedList and I am required to add a behavior which\nrotates a linked list left by N. (Assume, I am not doing TDD). I start with a behavior called rotateList(n int), build it and now I go on to adding a test.\nTest would look something like this -\n\nfunc TestRotatesALinkedListBy1(t *testing.T) {\n\tlinkedList := LinkedList{}\n\tlinkedList.rotateList(1)\n    ....\n}\n\nOne of the first things that I notice is the name of the method rotateList.\nIn the expression linkedList.rotateList, do I need to call the behavior as rotateList or just rotate is good enough\nbecause it is invoked on a list. I would go and rename it to rotate and now my client call looks like linkedList.rotate(1).\n\nOnce I start paying attention to the entire expression linkedList.rotate(1), I would realize rotate(1) is not making sense. It does not tell the clients or the readers that the intention is to\nrotate list by 1. What if it were renamed to rotateBy(), it would make my client call look like this linkedList.rotateBy(1)\n\nNow, probably the last thing would be, linkedList.rotateBy(1) does not tell the clients about the direction of rotation.\nWhat if it were renamed to rotateLeftBy(), it would make my client call look like this linkedList.rotateLeftBy(1)\n\nfunc TestRotatesALinkedListLeftBy1(t *testing.T) {\n\tlinkedList := LinkedList{}\n\tlinkedList.rotateLeftBy(1)\n    ....\n}\n\nBy not adding automated tests or by deferring addition of tests, we are just losing all these advantages.\n\nWould you buy a car without brakes?\n\nOne of the things that the automated tests provide is a “safety net” which in turn allows us to make changes in code with confidence. I am not sure why do we even call software delivery a delivery, without automated tests.\nLet’s try and draw an analogy between “building software” and “manufacturing car”.\n\n\n    If it is ok to write code without tests, to deliver software without tests, then we should be ok to buy a car without brakes. \n\n\nHow would it feel if a car manufacturer told us to buy a car without brakes. If adding tests in the code can be deferred or be considered non-essential, why can’t brakes in a car be considered non-essential?\nThe point is why can’t brakes be added later on if time permits, if there is no delivery pressure or, if there are less delivery orders?\n\nWe really need confidence in the car that we are driving, we also need confidence in the brakes, and if confidence is such an important thing, then\nwhy not build confidence in the code by adding automated tests for every piece of code that we write?\n\nIf our software is a car, then automated tests are the brakes. Like we said, we need confidence in the brakes as well which essentially means, it is not just about writing\ntests because of some code coverage policy in the organization, it is about “building a trail of understanding for all the readers”, about stating “what is it that a piece of code does”, “when does it fail”, “what kind of inputs does it take” and so many other things.\n\n\n\tIt is our responsibility to ensure these &quot;brakes&quot; exist in the code (or the system) and they are reliable. They should not exist just for the sake of existing. \n\n\nConclusion\n\nI don’t see any reason for not writing tests or deferring the addition of tests. I think once you get addicted to “quick feedback”, this point of developing\nwithout tests, writing code today and adding tests later and not doing TDD automatically goes away.\n\nIf a team finds it difficult to write tests or it takes too long to write tests, then the team needs to practice it more.\nPractice till it becomes a habit. Not adding tests is not a solution. It is an easy hack.\n\nIf a team believes their software has been working without issues and that too without tests, I think it is just a matter of “when”, not “if”. Try and\nget better at things before it all comes crashing down.\n\nIf a team believes there is delivery pressure today and tests can be added tomorrow, then the team needs to be sure of one thing - “That tomorrow is never coming”.\n"
} ,
  
  {
    "title"    : "Diving into Java Bytecode",
    "category" : "",
    "tags"     : " Java, JVM, Bytecode",
    "url"      : "/diving-into-java-bytecode/",
    "date"     : "April 4, 2021",
    "excerpt"  : "\n    Java code is compiled into an intermediate representation called &quot;bytecode&quot;. It is this bytecode which gets executed by JVM and is later converted into machine specific instructions by JIT compiler. With this article, we attempt to dive into ...",
  "content"  : "\n    Java code is compiled into an intermediate representation called &quot;bytecode&quot;. It is this bytecode which gets executed by JVM and is later converted into machine specific instructions by JIT compiler. With this article, we attempt to dive into bytecode and understand the internals of various bytecode operations.\n\n\nThis article aims to cover the following topics -\n\nContent\n\n  Terminology\n  Quick overview of class file structure\n  Bytecode execution model\n  Introducing bytecode opcodes\n  Opcodes for object creation\n  Combining things together\n  Summary\n  References\n\n\nLet’s get an understanding of some terms before we start to dive in.\n\nTerminology\n\nBytecode\n\nAn intermediate representation of Java code which JVM understands.\n\n\n    This intermediate representation is called bytecode because each &quot;opcode&quot; is represented by 1 byte. This effectively means, a total of 256 opcodes are possible. \n\n\nThese opcodes may take arguments and arguments can be up to 2 bytes long. This means a bytecode instruction which is a combination of an opcode and arguments could be as long as 3 bytes.\n\nWe will see various opcodes as we move on, but let’s take a quick glimpse of an instruction which is an output from javap utility -\n\n9: iconst_0\n\n\n  iconst_0 is an opcode which pushes a constant value 0 on top of the stack\n  Every opcode is prefixed with a letter like i / d etc to represent the data type that opcode is dealing with\n  Every bytecode instruction will start with an offset (9: in the previous example). This comes handy when a “goto” opcode is used\n\n\njavap\n\nStandard Java class file disassembler distributed with JDK. It provides a human-readable format of class file.\n\njavap -p -v -c &amp;lt;path to the class file&amp;gt;\n\n-p =&amp;gt; display private methods\n-v =&amp;gt; be verbose\n-c =&amp;gt; disassemble the source code\n\nQuick overview of class file structure\n\nLet’s take a quick look at the structure of the class file. Don’t worry if something is not clear at this stage, it should become clear as we proceed with examples.\nLet’s take a simple example to understand what constitutes our class file.\n\n(Please note: bytecode is trimmed for the entire article).\n\nfinal public class SumOfN implements Serializable {\n    private final int n;\n    public SumOfN(int n) {\n        this.n = n;\n    }\n    public int sum() {\n        //code left out\n    }\n}\n\nbytecode\n\npublic final class SumOfN implements java.io.Serializable\n    minor version: 0\n    major version: 59\n    flags: (0x0031) ACC_PUBLIC, ACC_FINAL, ACC_SUPER\n    this_class: #8                          // org/sample/SumOfN\n    super_class: #2                         // java/lang/Object\n    interfaces: 1, fields: 1, methods: 2, attributes: 1\nConstant pool:\n    #2 = Class              #4             // java/lang/Object\n    #4 = Utf8               java/lang/Object\n    #8 = Class              #10            // org/sample/SumOfN\n    #10 = Utf8              org/sample/SumOfN\n\nMagic number (0xCAFEBABE) is what every class file starts with. The first four bytes indicate that it is a class file and, the remaining four bytes\nindicate the minor and major versions used to compile the source file.\n\nMajor and minor version indicate the version of JDK used to compile the source file. In the previous example minor version is 0 and major version is 59 (which is Java SE 15).\n\nFlags indicate the modifiers that are applied to the class. In the previous example, we have ACC_PUBLIC indicating it is a public class,  ACC_FINAL indicating \nit is a final class, ACC_SUPER exists for backward compatibility for the code compiled by Sun’s older compilers for the Java programming language. (More on this here)\n\nConstant pool\nIs a part of class file which contains -\n\n  string literals/constants\n  doubles/float values\n  names of classes\n  interfaces\n  fields\n\n\nwhich are used in a class. Various opcodes like invokevirtual refer to constant pool entry to identify the virtual method to be invoked.\n\n1: invokevirtual #7  // Method run:()Ljava/lang/Object;\n\nHere, invokevirtual takes an argument which refers to an entry in the constant pool and, the entry indicates the method to be called along with its parameter and return type.\n\nthis_class refers to an entry (#8) in the constant pool, which in turn refers to another entry (#10) in the pool that returns org/sample/SumOfN.\nEffectively, this_class holds the name of the current class.\n\nsuper_class refers to an entry (#2) in the constant pool, which in turn refers to another entry (#4) in the pool that returns java.lang.Object \nEffectively, super_class holds the name of the super class.\n\ninterfaces, fields, methods respectively indicate the number of interfaces implemented by the class, number of fields that the class holds and the number of methods\nthat the class has.\n\nattributes are used in the class file, field level information, method information, and code attribute structures. One example of an attribute is Exceptions\nwhich indicates which checked exceptions a method may throw. This attribute is attached to method_info structure.\n\nThis was a very quick overview of class file structure, for more details please check this link.\n\nBytecode execution model\n\nJVM operates using stack as its execution model. Stack is a collection of frames, each of which is allocated when a method is invoked.\n\nA stack frame consists of -\n\nOperand Stack\nMost of the opcodes operate by pushing-in or popping-out value to or from the operand stack. Eg; iconst_0 pushes 0 on top of the stack.\n\nLocalVariableTable (an array of local variables)\nIn order to allow a variable to be assigned a value, a local variable table is used. LocalVariableTable is a simple data structure which contains the name of the variable, \nits data type, its slot along with some other fields.\n\nLocalVariableTable contains -\n\n  variables that are local to a method\n  method parameters\n  this, if the method is not static. this is allocated slot 0 in LocalVariableTable\n\n\nEg; istore_1 is an opcode which stores an integer value into LocalVariableTable at slot 1 by picking value from top of the stack.\n\nIntroducing bytecode opcodes\nLet’s take a simple example which adds 2 integers, to understand opcodes and their execution.\n\nAdditionExample\n\npublic class AdditionExample {\n    public int execute() {\n        int addend = 10;\n        int augend = 20;\n        return addend + augend;\n    }\n}\n\nbytecode (AdditionExample)\n\npublic class AdditionExample {\n    public AdditionExample();\n        Code:\n            0: aload_0\n            1: invokespecial #1   // Method java/lang/Object.&quot;&amp;lt;init&amp;gt;&quot;:()V\n            4: return\n    \n    public int execute();\n        Code:\n          stack=2, locals=3, args_size=1\n            0: bipush        10\n            2: istore_1\n            3: bipush        20\n            5: istore_2\n            6: iload_1\n            7: iload_2\n            8: iadd\n            9: ireturn\n\n        LocalVariableTable:\n        Start  Length  Slot  Name     Signature\n            0      10     0  this     Lorg/sample/AdditionExample;\n            3       7     1  addend   I\n            6       4     2  augend   I\n}\n\nThe bytecode of AdditionExample() should become clear as we move on but first let’s understand the bytecode of execute method -\n\n  The Java compiler has indicated the depth of stack needed during the execution of this method. stack=2 means at any point during this method execution\nwe will have a maximum of 2 entries on the stack. locals=3 indicate that there are 3 local variables which will need to go in LocalVariableTable. One variable is\naddend, other is augend and the last is this. args_size=1 indicates one object needs to be initialized before the method call, which \nagain is this\n  bipush is an opcode which pushes a byte sized integer on the stack. It takes an argument which is 10 in our case\n  istore_1 takes the value from top of the stack, which is 10 and assigns it into LocalVariableTable at slot 1. This opcode removes the value from stack top\n  bipush now pushes 20 to the top of the stack\n  istore_2 takes the value from top of the stack, which is 20 and assigns it into LocalVariableTable at slot 2\n  At this stage, values 10 and 20 have been assigned to addend and augend in LocalVariableTable, and our stack is empty. This means these 2 values need to be brought into stack before an addition can be performed\n  iload_1 copies the value from slot 1 of LocalVariableTable to the stack\n  iload_2 copies the value from slot 2 of LocalVariableTable to the stack\n  Stack now contains 10 and 20. iadd pops 2 integer values from top 2 positions of the stack and sums them up. It stores the result back in the stack top\n  ireturn takes the value from stack top and returns an integer\n\n\nFollowing diagram represents the overall execution -\n\n    \n\n\n\nFew things to note-\n\n  All the opcodes are prefixed with an i, indicating that we are dealing with an integer data type\n  Slot 0 of LocalVariableTable is occupied by this of AdditionExample\n  All the entries in LocalVariableTable are statically typed\n  Bytecode is statically typed in a sense that all the opcodes which work with specific data type\n\n\nQuick summary of opcodes that we have seen so far -\n\n\n  \n    \n      Opcode\n      Purpose\n    \n  \n  \n    \n      istore_slot\n      Takes an integer value from top of the stack and assigns it into LocalVariableTable at defined slot\n    \n    \n      iload_slot\n      Copies the value from defined slot of LocalVariableTable to the stack\n    \n    \n      bipush\n      Pushes a byte sized integer on the stack\n    \n  \n\n\nLet’s take another example, which is a slight modification of the first one. The idea is to invoke a method from another method.\n\nMethodInvocation\n\npublic class AdditionExample {\n    public int execute() {\n        return add();\n    }\n    private int add() {\n        int addend = 10;\n        int augend = 20;\n        return addend + augend;\n    }\n}\n\nbytecode (MethodInvocation)\n\npublic class AdditionExample {\nConstant pool:\n    #7 = Methodref          #8.#9          // org/sample/AdditionExample.add:()I\n    #8 = Class              #10            // org/sample/AdditionExample\n    #9 = NameAndType        #11:#12        // add:()I\n    #10 = Utf8              org/sample/AdditionExample\n    #11 = Utf8              add\n    #12 = Utf8              ()I\n\n    public AdditionExample();\n        Code:\n            0: aload_0\n            1: invokespecial #1    // Method java/lang/Object.&quot;&amp;lt;init&amp;gt;&quot;:()V\n            4: return\n        \n    public int execute();\n        Code:\n            0: aload_0\n            1: invokevirtual #7   // Method add:()I\n            4: ireturn\n    \n    private int add();\n        Code:\n            0: bipush        10\n            2: istore_1\n            3: bipush        20\n            5: istore_2\n            6: iload_1\n            7: iload_2\n            8: iadd\n            9: ireturn\n}\n\nBytecode in add method should look very familiar 😁. Let’s look at the bytecode for execute method which only invokes add method-\n\n  aload_0 copies the value from slot 0 of LocalVariableTable to the stack. Slot 0 of LocalVariableTable contains this, which means stack top now contains this\n  Now is the time to invoke the private method add of the same class. invokevirtual is used for invoking a virtual method and, it takes a parameter which is a reference to an entry in the constant pool. Let’s see how does this entry get used -\n    \n      Entries in constant pool are composable, which means an entry could be created by referring to other entries\n      #7 is a method reference entry which refers to #8 and #9\n      #8 refers to an entry #10 which specifies the name of the class org/sample/AdditionExample\n      #9 refers to entries #11 and #12 which specify the method name add along with its signature ()I (no parameters, integer return type) respectively\n      #7 in short, provides a complete signature of the add method including its class name org/sample/AdditionExample.add:()I\n    \n  \n  Our stack contains this which will be used for invoking add method\n  invokevirtual pops the entry from stack top which is this, invokes add method and stores the result in stack top\n  ireturn takes the value from stack top and returns an integer\n\n\njavap by default does not return the output for private methods, use -p flag to see the output for private methods.\n\nOne of the questions that is worth answering is “how does invokevirtual know about the number entries to be popped out?”. In order to answer this, we will modify our\nprevious example slightly and see the behavior of invokevirtual.\n\nMethodInvocation with parameters\n\npublic class AdditionExample {\n    public int execute() {\n        return add(10, 20);\n    }\n    private int add(int addend, int augend) {\n        return addend + augend;\n    }\n\nbytecode (MethodInvocation with parameters)\n\npublic class AdditionExample {\n    public AdditionExample();\n        Code:\n            ....\n    \n    public int execute();\n        Code:\n            0: aload_0\n            1: bipush        10\n            3: bipush        20\n            5: invokevirtual #7       // Method add:(II)I\n            8: ireturn\n    \n    private int add(int, int);\n        Code:\n            ...\n}\n\nLet’s look at the bytecode for execute method again -\n\n  this is pushed on the stack, followed by push of values 10 and 20\n  Stack contains this, 10 and 20\n  There is a change in signature of the method which will be invoked by invokevirtual. add now takes 2 integer parameters and returns an integer. Method signature is denoted by add:(II)I\n  invokevirtual now needs to pop 3 entries from the stack, 2 integers which were pushed using bipush opcode and a reference to this which was pushed using aload_0\n  Once it pops the entries, add method is invoked and, the result is stored in stack top\n  ireturn takes the value from stack top and returns an integer\n\n\nEffectively, invokevirtual knows the number of entries to be popped based on the signature of the method to be invoked. As seen in previous example, in order to invoke a method\nwhich takes 2 parameters, we need to pop 2 values from the stack along with an instance of the current class.\n\nQuick summary of opcodes that we have seen so far -\n\n\n  \n    \n      Opcode\n      Purpose\n    \n  \n  \n    \n      aload_slot\n      Copies the address value from a defined slot of LocalVariableTable to the stack, a stands for address\n    \n    \n      invokevirtual\n      Invokes virtual method, pops the entries from stack based on the signature of the method to be invoked\n    \n  \n\n\nOpcodes for object creation\nLet’s take an example to understand the bytecode that gets generated during object creation.\n\npublic class Book {\n    public Book(String name, Date publishingDate) {\n        ///\n    }\n    public Date publishingDate() {\n        return new Date();\n    }\n}\n\nThis example uses java.util.Date, (don’t ask why) and returns a new Date as a part of publishingDate method (again, don’t ask why 😁).\n\nbytecode (Object creation)\n\npublic class Book {\npublic java.util.Date publishingDate();\n    Code:\n        0: new           #7     // class java/util/Date\n        3: dup\n        4: invokespecial #9     // Method java/util/Date.&quot;&amp;lt;init&amp;gt;&quot;:()V\n        7: areturn\n}\n\nThis is a new territory that we are going into. Let’s understand the bytecode -\n\n  new allocates the required memory for the object but does not call the constructor. It refers to the constant pool and identifies the object which is java/util/Date here, and allocates the required memory\n  Our stack now contains the object reference created by new\n  Before we understand dup, let’ understand the need for it -\n    \n      Let’s assume there is no dup\n      Our stack contains an object reference which means it is referring to some memory allocated by new opcode\n      So far our date object has not been initialized. We need another opcode (invokespecial) for initializing it\n      invokespecial is used for invoking special methods like constructors. invokespecial refers to an entry in the constant pool (#9), resolves it\nto the init method of java.util.Date class.\n      invokespecial will pop the entry from stack top and invoke the init method of java.util.Date. This means our date object is fully initialized now\n      But, now our stack does not contain any reference to the newly created object because it was popped by invokespecial to invoke a method which does not return anything\n    \n  \n  So, we need dup to duplicate the entry on stack top\n  invokespecial pops the entry from stack top, invokes the init method of java.util.Date class to initialize the object\n  We now have the stack containing an object reference which refers to the fully created java.util.Date instance\n  areturn takes the value from stack top and returns java.util.Date address\n\n\nFollowing diagram represents the overall execution -\n\n    \n\n\n\nQuick summary of opcodes that we have seen so far -\n\n\n  \n    \n      Opcode\n      Purpose\n    \n  \n  \n    \n      new\n      Allocates the required memory for the object does not call the object constructor\n    \n    \n      dup\n      Duplicates the entry present on stack top\n    \n    \n      invokespecial\n      Invokes special methods like constructors\n    \n  \n\n\nCombining things together\nTime to take one last example and validate our learning.\n\npublic class SumOfN {\n    private final int n;\n    public SumOfN(int n) {\n        this.n = n;\n    }\n    public int sum() {\n        int sum = 0;\n        for (int number = 1; number &amp;lt;= n; number++) {\n            sum = sum + number;\n        }\n        return sum;\n    }\n}\n\nbytecode (SumOfN) constructor\n\npublic class SumOfN {\n    private final int n;\n    \n    public SumOfN(int);\n        Code:\n            0: aload_0\n            1: invokespecial #1     // Method java/lang/Object.&quot;&amp;lt;init&amp;gt;&quot;:()V\n            4: aload_0\n            5: iload_1\n            6: putfield      #7     // Field n:I\n            9: return\n        \n        LocalVariableTable:\n        Start  Length  Slot  Name   Signature\n            0      10     0  this   Lorg/sample/SumOfN;\n            0      10     1     n   I\n}\n\nLet’s begin with SumOfN(int) constructor and understand the bytecode. Instead of going through the code first, let’s try and, figure out what might the bytecode look like by\nunderstanding what needs to be done.\n\n\n  \n    What needs to be done\n    How can it be done\n  \n  \n    We should be able to invoke the constructor of java.lang.Object\n    load this reference on the stack, which is what aload_0 does\n  \n  \n    invoke init method of java.lang.Object which is what invokespecial does. It pops this reference from stack top\n  \n\n  \n    We should be able to store the value of n in class field\n    load this reference on the stack, which is what aload_0 does\n  \n  \n    load the value of n on the stack from LocalVariableTable, which is what iload_1 does. n has slot 1 in LocalVariableTable\n  \n  \n    put the value of n in class field, which is what putfield does. It pops the 2 entries from stack top, one is this and other is the value of n and sets the class field\n  \n\n\nAll these opcodes make up our constructor. Let’s now jump to the sum method.\n\nbytecode (SumOfN) sum method\n\npublic int sum();\n    Code:\n        0: iconst_0\n        1: istore_1\n        2: iconst_1\n        3: istore_2\n        4: iload_2\n        5: aload_0\n        6: getfield      #7     // Field n:I\n        9: if_icmpgt     22\n        12: iload_1\n        13: iload_2\n        14: iadd\n        15: istore_1\n        16: iinc          2, 1\n        19: goto          4\n        22: iload_1\n        23: ireturn\n\n    LocalVariableTable:\n    Start  Length  Slot  Name   Signature\n        4      18     2  number   I\n        0      24     0  this     Lorg/sample/SumOfN;\n        2      22     1  sum      I\n\n\n  \n    What needs to be done\n    Code snippet\n    How can it be done\n  \n  \n    Initialize sum with a value 0\n    int sum = 0\n    iconst_0 and istore_1 should be able to put 0 on the stack and assign it to local variable sum. sum variable has slot 1 in LocalVariableTable\n  \n  \n    Initialize number with a value 1\n    int number = 1\n    iconst_1 and istore_2 should be able to put 1 on the stack and assign it to local variable number. number variable has slot 2 in LocalVariableTable\n  \n  \n    We should be able to compare the value of number and the value of the class field n. In order for this to happen, we need to load the \n   value of number and this reference on the stack. We need this reference to be able to get the value of instance variable n\n    number &amp;lt;= this.n\n    iload_2 and aload_0 should be able to copy the value of number variable from slot 2 and this reference from slot 0 on the stack\n  \n  \n    We should be able get the value of class field n\n    this.n\n    getfield should be able to help here. It pops the class instance to get the field value. The field value goes on the stack. Now, our stack contains value of number and n\n   \n  \n    Perform the required comparison. If the condition indicates exit from the loop, return the value present on the stack top\n    number &amp;lt;= this.n\n    if_icmpgt does the integer comparison. It pops the top 2 integer values from the stack and does the comparison (number &amp;gt; n). If condition returns true, it takes an argument which is the instruction offset to jump to\n   \n  \n    If the condition indicates loop continuation, load the value of sum and number variable to be able to perform addition\n    sum + number\n    iload_1 and iload_2 should do it. Now our stack has 2 values which are ready for addition\n   \n  \n    Perform addition\n    sum + number\n    iadd does the integer addition. Pops the top 2 values from the stack and puts the result back in the stack\n  \n  \n    Assign the result of addition to the variable sum\n    sum = sum + number\n    istore_1 would do the job. It takes the value from stack top and assign the value in variable sum\n  \n  \n    Increment the value of number\n    number = number + 1\n    iinc does integer increment and takes 2 parameters. First one is the LocalVariableTable slot and other one is the increment. It is one of the opcodes that does not work \n    with stack. It increments the value at a specific slot in LocalVariableTable\n  \n  \n    Repeat steps\n    NA\n    goto is the opcode which transfers the control to a specific instruction set\n  \n\n\nFollowing diagram represents the overall execution of sum method -\n\n    \n\n\n\nSummary\nLet’s conclude with some key takeaways -\n\n\n  javap provides a human-readable format of class file\n  Each opcode in a bytecode is represented by 1 byte\n  Each opcode is prefixed with a letter indicating the data type the opcode will work with\n  Most of the opcodes work with the stack which means before they operate, values need to be brought on the stack\n  Opcode like iinc works with LocalVariableTable instead of working with values on the stack\n  Opcodes like invokevirtual, invokespecial refer to an entry in the constant pool to resolve the method that needs to be invoked\n  Some opcodes also have shortcuts. eg; iconst_0 pushes 0 on the stack without taking any argument. It could have been designed to take an argument \nbut that would have meant the total instruction size will be greater than 1 byte (1 byte for the opcode and another byte for the argument). In order to avoid\nincreasing the size of the instruction, it is designed in a shortcut form\n\n\nHope it was meaningful. Appreciate the feedback.\n\nReferences\n\n  Advanced Java Bytecode Tutorial\n  Java Bytecode: Using Objects and Calling Methods\n  Java Bytecode Crash Course\n  Optimizing Java - practical techniques for improving JVM application performance\n\n"
} ,
  
  {
    "title"    : "AWS Lambda - A Virtual Podcast",
    "category" : "",
    "tags"     : " AWS Lambda, Serverless",
    "url"      : "/aws-lambda-a-virtual-podcast/",
    "date"     : "April 19, 2020",
    "excerpt"  : "\n    AWS Lambda is a serverless compute service and after having worked with it for sometime, I felt it is a good time for me to share my learning and experiences. I have been thinking of writing an article in a &quot;Virtual Podcast format&quot; and felt t...",
  "content"  : "\n    AWS Lambda is a serverless compute service and after having worked with it for sometime, I felt it is a good time for me to share my learning and experiences. I have been thinking of writing an article in a &quot;Virtual Podcast format&quot; and felt this could be the one.\n\n\nWelcome all to this article named AWS Lambda - A Virtual Podcast and let me introduce our guests Mr. Hernandez and Ms. Jessica who would walk us through their experiences of using AWS Lambda.\n\nWelcome, Hernandez and Jessica and thank you for participating in this Virtual Podcast. Let’s get started.\n\nWhat is AWS Lambda\n\nMe&amp;gt; My first question to you Jessica is “What is AWS Lambda?”\n\nJessica&amp;gt; AWS Lambda is a serverless compute service which allows you to execute a function in response to various events without provisioning or managing servers. What this means is your function will execute ONLY when there is a request for it.\n\nMe&amp;gt; So what I gather is, a function is an entry point which gets invoked by AWS Lambda service. Is that right?\n\nJessica&amp;gt; That’s nearly right. When you create a lambda function, you need to specify a handler which is nothing but the filename.exported function name that acts as an entry point for your application.\n\nLet’s say, you have a file named “handler.js” and it exports a function named “processOrders”, your handler becomes handler.processOrders which will be invoked by AWS Lambda in response to events.\n\nMe&amp;gt; Thank you Jessica.\n\n\n    AWS Lambda is a serverless compute service which allows you to execute a function in response to various events without provisioning or managing servers. When you create a lambda function, you need to specify a handler which acts as an entry point for your lambda function.\n\n\n. . . \n\nHow does a lambda function execute?\nMe&amp;gt; Jessica, you mentioned that a lambda function runs in response to an event, but where does it run?\n\nJessica&amp;gt; When you create a lambda function, you need to specify a runtime say, node12.x, python3.7 or anything else. When there is a request for your lambda function, AWS will provision a container with the selected runtime and then run your function.\n\nMe&amp;gt; So it is actually a container within which a lambda function is run. Does that also mean your lambda function gets some storage on file system?\n\nJessica&amp;gt; Yes, your lambda function gets around 500MB of storage in /tmp directory but that is ephemeral. It goes away as the container goes away.\n\n\n    AWS will provision a container to run your function when there is a request for your lambda function. This container will be discarded after some inactive time.\n\n\n. . . \n\nWhat is AWS Lambda Cold Start\n\nMe&amp;gt; Hernandez, since a lambda function is not always running, does it increase the response time of a request?\n\nHernandez&amp;gt; Like Jessica mentioned, a lambda function will run inside a container which will stay active till the time your function is running. This container will be discarded by AWS after some inactive time thus making your function inactive and this is called as cold state.\n\nWhenever there is a request for a cold function, AWS needs to provision a container for running your function and this is called as Cold Start. So, to answer your question, yes, cold start can add to the response time of a request.\n\nMe&amp;gt; Is there a way to avoid cold start?\n\nHernandez&amp;gt; Yes. AWS has now introduced Provisioned Concurrency which is designed to keep your functions initialized and ready to respond in double-digit milliseconds at the scale you need. Provisioned concurrency adds pricing dimension though.\n\nYou can turn it ON/OFF from AWS console or CloudFormation template.\n\nIf you are using serverless framework you should checkout this blog for keeping your functions warm.\n\nMe&amp;gt; Thank you Hernandez.\n\n\n    AWS needs to provision a container for running your cold function and this is called as Cold Start. You should check Provisioned Concurrency (or even Serverless plugin WarmUP) for keeping your functions initialized.\n\n\n. . . \n\nAWS Lambda Configuration\n\nMe&amp;gt; Jessica, what are the different configuration options one can specify while creating a lambda function?\n\nJessica&amp;gt; You can specify a lot of options including -\n\n  IAM role\n  Memory, which ranges from 128MB to 3GB\n  Timeout, which ranges from 1sec to 15mins\n  Environment variables, which can be upto 4KB in size\n  VPC configuration for executing your function inside a VPC\n  Concurrency\n\n\nMe&amp;gt; Wow, these are too many. Jessica you mentioned memory, but no mention of CPU?\n\nJessica&amp;gt; Yes, you can not control the amount of CPU that gets allocated to your lambda function, it is actually proportional to the amount of memory allocated.\n\nMe&amp;gt; I see. Jessica, what do you mean by Concurrency of a lambda function?\n\nJessica&amp;gt; I like the example given in Managing AWS Lambda Function Concurrency. Imagine each slice of a pizza is an execution unit of a lambda function and the entire pizza represents the shared concurrency pool for all lambda functions in an AWS account.\n\nLet’s say, we set concurrency limit of 100 for a lambda function, all we are saying is the lambda function will have a total of 100 pizza slices which means you can have 100 concurrent executions of lambda function. Concurrency limit set for a lambda function is reduced from concurrency pool, which is 1000 for all lambda functions per AWS account - the entire pizza.\n\nMe&amp;gt; Jessica, I also see an option of Unreserved Concurrency in lambda configuration. What is that?\n\nJessica&amp;gt; AWS also reserves 100 units of concurrency for all functions that don’t have a specified concurrency limit set. This helps to make sure that future functions have capacity to be consumed.\n\nMe&amp;gt; Thank you Jessica. I am starting to wonder what happens when a lambda function’s concurrency limit is reached and there are more requests?\n\nJessica&amp;gt; Lambda function gets throttled.\n\nMe&amp;gt; Does that mean a client of your lambda function say API Gateway will get an error?\n\nJessica&amp;gt; It actually depends on the type of request. If it a synchronous request, it will end with a timeout error.\n\nWhereas in case of asynchronous request, say from SQS, AWS Lambda will retry your lambda function before sending the request event to a Dead Letter Queue, assuming one is configured.\n\n\n    Various configuration options can be specified while creating a lambda function including IAM role, memory, timeout, VPC concurrency etc.\n\n\n. . . \n\nAWS Lambda Debugging\n\nMe&amp;gt; Hernandez, what AWS services can help us with debugging an issue with a lambda function?\n\nHernandez&amp;gt; AWS Lambda function logs are sent to CloudWatch and lambda function needs an IAM role in order to that. Other than CloudWatch, you can also use AWS X-Ray for tracing and debugging performance issues.\n\nMe&amp;gt; Nice. How to set up AWS X-Ray with lambda function? Do you need to set up an X-Ray agent or something like that?\n\nHernandez&amp;gt; No, with lambda function, you need to do a very few things in order to set up tracing -\n\n\n  Set up an IAM role in order to send traces to AWS X-Ray\n  Enable ActiveTracing either in AWS console or CloudFormation\n  Use AWS X-Ray SDK in your lambda function code\n\n\nRest everything is taken care by AWS Lambda.\n\nMe&amp;gt; Ok. Once this is done, AWS will be able to build a service map signifying which services were invoked by lambda function and indicate the problems, if any. Is that right?\n\nHernandez&amp;gt; Yes, that is right.\n\n\n    AWS Lambda function logs are sent to CloudWatch and lambda function needs an IAM role in order to that. Other than CloudWatch, you can also use AWS X-Ray for tracing and debugging performance issues.\n\n\n. . . \n\nRestrictions with AWS Lambda\n\nMe&amp;gt; Jessica, any restrictions around AWS Lambda that we should be aware of?\n\nJessica&amp;gt; I think there are a few restrictions -\n\n\n  Maximum unzipped code size for lambda function can be 250MB\n  Environment variables can be a maximum of 4KB in size\n  Maximum timeout of a lambda function can be 15mins\n  Maximum amount of memory that can be allocated to a lambda function can be 3GB\n  A lambda function can have a total of 5 lambda layers\n  Not all runtime or programming languages are supported by AWS Lambda\n\n\nWith that said, I feel you might not hit all these limitations. To elaborate, if your unzipped code size is going beyond 250MB, I think it is good to understand why is a lambda function getting too huge. Have we packed too many dependencies or have \nwe mixed too many responsibilities in a lambda function or is it something else.\n\nMe&amp;gt; Jessica, what is lambda layer?\n\nJessica&amp;gt; A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies needed by your application. With layers, you can use libraries in your function without needing to include them in your deployment package. Layers let you keep your deployment package small.\n\nMe&amp;gt; Ok, then 5 lambda layers in an application looks like a sensible default.\n\nJessica&amp;gt; True. I think these constraints are very sensible and if we are hitting some of them, it is worth looking back and seeing if there is a problem somewhere else.\n\n\n    AWS Lambda has some restrictions and our panel feels these are sensible restrictions. It is good to know them.\n\n\n. . . \n\nUnit and Integration Testing with AWS Lambda\n\nMe&amp;gt; Coming to my favorite topic. How has your experience been with testing of AWS Lambda function?\n\nJessica&amp;gt; Well, unit testing is not difficult. If you are coding your lambda function in typescript, you can very well use sinon to mock all the dependencies and just validate that a single unit is working fine.\n\nHernandez&amp;gt; True. I think challenge comes when you want to assert that the integration of your lambda function with external systems say DynamoDB or S3 works properly. In order to test this, we have used LocalStack in our project.\n\nMe&amp;gt; LocalStack? Do you want to talk a bit about this?\n\nHernandez&amp;gt; Sure. LocalStack provides an easy-to-use test/mocking framework for developing Cloud applications. At this stage, their focus is primarily on supporting the AWS cloud stack.\n\nLocalStack spins up various Cloud APIs on local machine including S3, lambda, DynamoDB and API Gateway. All you need to do is, spin up LocalStack docker container, deploy your infra say Dynamo table or lambda function within LocalStack and connect to these services running on local machine from within your code.\n\nMe&amp;gt; Interesting. Does LocalStack support all AWS services?\n\nHernandez&amp;gt; No, it supports quite a few but definitely not all.\n\n\n    I am sure Unit testing with AWS Lambda function code is understood by all of us but what is good to know is LocalStack can be used for integration testing.\n\n\n. . . \n\nPackaging and deploying an AWS Lambda application\n\nMe&amp;gt; Jessica, you talked about unzipped code. Does that mean you have to create a zip file and upload it somewhere?\n\nJessica&amp;gt; Well, you have package your lambda function along with its dependencies as an archive, upload it either on AWS Lambda console or in an S3 bucket which will be referenced from your CloudFormation template.\n\nMe&amp;gt; How do you folks package your application? It appears to me as if we need to create a “fat jar” kind of a thing.\n\nHernandez&amp;gt; We use typescript for coding our lambda application and webpack for packaging it. It does not create a zip file, just an out directory containing the transpiled code (js) and a handler.js file with all the required code from different node_modules plus its source map.\n\nMe&amp;gt; How do you deploy your code then because you just seemed to create an output directory with a few javascript files.\n\nHernandez&amp;gt; We use CDK for deploying our code which allows you to code your infra.\n\nMe&amp;gt; Wow, the list of tools doesn’t seem to come to an end.\n\nHernandez&amp;gt; It’s simple. Just look at it this way, we have just created a directory which is ready to be deployed and moment you say cdk bootstrap, it will copy the contents of this out directory into another directory which will be archived and uploaded to an S3 bucket.\n\nAnd when you say cdk deploy, you will see all the required AWS components getting deployed. Simple.\n\nMe&amp;gt;Simple? You said contents of this out directory will be copied into another directory. Does that mean CDK already knows about the out directory?\n\nHernandez&amp;gt; That’s true. When you code your infra, you will specify where is your compiled (or transpiled) or ready to be shipped code located and that’s how CDK knows about this directory.\n\nMe&amp;gt; Great, now I able to connect dots. Build your code -&amp;gt; get a shippable directory -&amp;gt; archive it -&amp;gt; upload it to an S3 bucket -&amp;gt; deploy it and CDK is one way to get all these steps done. Is that right?\n\nHernandez&amp;gt; Absolutely.\n\n\n    In order to deploy your your lambda function, it needs to be packaged along with its dependencies as an archive. You could use webpack if you are using typescript as a programming language. You can use CDK, CloudFormation or SAM for packaging and deploying your lambda function.\n\n\n. . . \n\nApplications built using AWS Lambda\n\nMe&amp;gt; Jessica, Hernandez, what are the different types of applications that you folks have built using AWS Lambda?\n\nJessica&amp;gt; We have actually built serverless microservices using AWS Lambda and we also process web clicks on our application which is a stream of events flowing from user interface to AWS Pinpoint to AWS Kinesis to AWS Lambda.\n\nHernandez&amp;gt; We use AWS Lambda for scaling down images that are uploaded to our S3 buckets and for processing DynamoDB streams which is a stream of changes in DynamoDB table.\n\nMe&amp;gt; Thanks Jessica and Hernandez.\n\n\n    Our panel highlighted different types of applications they have built using AWS Lambda including microservices, event processing (images on S3 buckets) and stream processing (web clicks and handling changes in DynamoDB).\n\n\n. . . \n\nWith this we come to an end of our “Virtual Podcast” and a big Thank you to Jessica and Hernandez for being a part of this. This was wonderful, and hope our readers (yes, it is still virtual) find it the same way. Thank you again.\n\nReferences\n\n  Managing AWS Lambda Function Concurrency\n  Keeping Functions Warm - How To Fix AWS Lambda Cold Start Issues\n  Provisioned concurrency\n  Lambda Layer\n\n"
} ,
  
  {
    "title"    : "Concluding Serverless Journey",
    "category" : "",
    "tags"     : " AWS Lambda, Serverless",
    "url"      : "/concluding-serverless-journey/",
    "date"     : "March 18, 2020",
    "excerpt"  : "\nWe have come a long way in our Serverless journey.\n    This journey which started with building a serverless application has finally come to a stage where we can see all\n    our hard work in action. Yes, we will be deploying our application.\n\n\nWe...",
  "content"  : "\nWe have come a long way in our Serverless journey.\n    This journey which started with building a serverless application has finally come to a stage where we can see all\n    our hard work in action. Yes, we will be deploying our application.\n\n\nWe will be using AWS CDK to deploy our\n    application. Before we start using CDK, let&#39;s quickly look at what is CDK -\n\n\n\n    \n        The AWS Cloud Development Kit (AWS CDK) is an open\n            source software development framework to model and provision your cloud application resources using\n                familiar programming languages.\n            Provisioning cloud applications can be a challenging process that requires you to perform manual actions,\n                write custom scripts, maintain templates, or learn domain-specific languages. \n            AWS CDK uses the familiarity and expressive power of programming languages for modeling your\n                applications. It provides you with high-level components that preconfigure cloud resources with\n                proven defaults, so you can build cloud applications without needing to be an expert. \n            AWS CDK provisions your resources in a safe, repeatable manner through AWS\n                CloudFormation. It also enables you to compose and share your own custom\n                components that incorporate your organization&#39;s requirements, helping you start new projects\n                faster. https://aws.amazon.com/cdk/\n        \n        \n\n\n\nIn summary, we don&#39;t have to directly deal with CloudFormation or SAM for deploying our application. We will\n    provision our cloud resources using a higher level framework called CDK which will ultimately translate into a\n    CloudFormation template. \n\n\nWe should be able to see the advantages of using CDK very soon but let&#39;s look at this conversation to get some\n    understanding of CDK.\n\n\n\n    \n        \n    \n\nLet&#39;s begin now.\n\n\nStep 1: Setting up the project\n\n\nWe will be using the same project which was pushed here. \n\n\n\n    Install CDK globally by executing npm install aws-cdk -g\n    Create a directory named infra inside our project serverless-order-service\n    Execute cdk init app --language=typescript inside infra directory\n\n\n\nThis should generate a project which uses typescript as the programming language and jest\n    as a testing framework. Let&#39;s update the generated jest.config.js.\n\n\nBelow is how our jest.config.js will look like -\n\n\nmodule.exports = {\n    &quot;testMatch&quot;: [\n        &quot;**/__tests__/**/*.+(ts|tsx|js)&quot;,\n        &quot;**/?(*.)+(spec|test).+(ts|tsx|js)&quot;\n    ],\n    &quot;transform&quot;: {\n        &quot;^.+\\\\.(ts|tsx)$&quot;: &quot;ts-jest&quot;\n    },\n};\n\n\n\n    \n\nIf all has gone well so far this how our project structure will look like -\n\n    infra-stack.ts defines a class called InfraStack which is going to be a logical collection of\n        various constructs like lambda function(s), dynamodb etc\n    \n    infra.ts is the entry point of the application which creates an instance of InfraStack\n    infra.test.ts contains a simple test to assert an empty stack\n    package.json contains the project definition along with various dependencies including @aws-cdk/assert\n        which is a library for asserting various cloud resources\n    \n    jest.config.js contains the necessary configuration to run jest tests\n    cdk.json contains the command to run cdk application\n\n\n\nLet&#39;s make a few quick changes to the file names to match our convention, run the test and commit the changes -\n\n\n\n    Rename infra.ts to OrderServiceInfra.ts\n    Rename infra-stack.ts to OrderServiceInfraStack.ts\n    Rename infra.test.ts to OrderServiceInfraStack.spec.ts\n\n\n\nStep 2: Creating stack with lambda function\n\n\nLet&#39;s provision our lambda function. In order to do so we need to add a dependency @aws-cdk/aws-lambda.\n    So, let&#39;s add it by executing npm install @aws-cdk/aws-lambda@1.19.0.\n\n\nWe will start by creating a lambda function construct inside OrderServiceInfraStack.\n\n\nimport * as cdk from &quot;@aws-cdk/core&quot;;\nimport {Function} from &quot;@aws-cdk/aws-lambda&quot;;\n\nexport class OrderServiceInfraStack extends cdk.Stack {\n    constructor(scope: cdk.Construct, id: string, props?: cdk.StackProps) {\n        super(scope, id, props);\n\n        //create a lambda function in the stack\n        new Function(this, &quot;order-service-function&quot;, null); //compilation error\n    }\n}\n\n\nFew quick observations - \n\n\n\n    We have imported Function class from @aws-cdk/aws-lambda\n    Constructor of Function class takes 3 parameters -\n        \n            scope: Construct - which identifies the parent resource\n            id: string - unique identifier of the resource within the stack\n            props: FunctionProps - lambda function properties including name, runtime, handler etc\n        \n    \n    Typescript compiler gives an error because null is not acceptable in place of FunctionProps\n\n\n\nLet&#39;s pass the required function properties - \n\n\nimport {Code, Function, FunctionProps, Runtime} from &quot;@aws-cdk/aws-lambda&quot;;\nimport {Construct, Stack, StackProps} from &quot;@aws-cdk/core&quot;;\n\nexport class OrderServiceInfraStack extends Stack {\n    constructor(scope: Construct, id: string, props?: StackProps) {\n        super(scope, id, props);\n\n        //create FunctionProps\n        const functionProperties: FunctionProps = {\n            code: Code.fromAsset(&quot;../dist&quot;),\n            handler: &quot;handler.ordersHandler&quot;,\n            runtime: Runtime.NODEJS_10_X,\n            functionName: &quot;order-service-function&quot;,\n            environment: {&quot;ExecutionEnvironment&quot;: &quot;dev&quot;}\n        };\n        //create a lambda function in the stack\n        new Function(this, &quot;order-service-function&quot;, functionProperties);\n    }\n}\n\n\nHere, code, handler and runtime are the only mandatory properties. Passing them should make the compiler\n    happy. \n\n\nWith this change in OrderServiceInfraStack, our test will break because it asserts for empty resources inside the\n    stack but now stack contains a lambda function. We will fix the test in a moment.\n\n\nQuick observation - \n\n\n\n    We are passing ExecutionEnvironment as lambda environment variable. This variable is used to\n        determine if the lambda is running is running in test mode or production mode. This value can also be taken as\n        deployment parameter, but for now we are passing it as dev\n    \n    We have used ../dist inside code asset which contains our transpiled code\n\n\n\nStep 3: Fixing the test\n\n\nCDK allows us to write different forms of tests including snapshot tests and fine grained unit tests. We will be\n    writing both the tests - snapshot test(s) for our entire stack and unit tests for resources like lambda function,\n    dynamodb, api gateway etc.\n\n\nWe will be starting with unit tests which will assert on a resource and its properties.\n\n\nimport {OrderServiceInfraStack} from &quot;../lib/OrderServiceInfraStack&quot;;\nimport {App} from &quot;@aws-cdk/core&quot;;\nimport {Runtime} from &quot;@aws-cdk/aws-lambda&quot;;\nimport &quot;@aws-cdk/assert/jest&quot;;\n\ntest(&quot;stack should contain a lambda function with node10 as the runtime&quot;, () =&amp;gt; {\n    const app = new App();\n    const stack = new OrderServiceInfraStack(app, &quot;OrderServiceStack&quot;); //instantiate stack\n\n    //assert that stack contains a lambda function with node10 as the runtime\n    expect(stack).toHaveResource(&quot;AWS::Lambda::Function&quot;, {\n        Runtime: Runtime.NODEJS_10_X.toString()\n    })\n});\n\n\nFew quick observations - \n\n\n\n    We have imported aws-cdk/assert/jest which provides us with expect function that\n        allows us to match resources in the stack\n    \n    Our unit test asserts only on lambda&#39;s runtime property\n\n\n\nThat&#39;s it. Our lambda function resource is created in the stack and we have been able to write a unit test. Let&#39;s\n    commit the changes.\n\n\nStep 4: Adding DynamoDB to stack\n\n\nLet&#39;s provision dynamodb. In order to do so we need to add a dependency @aws-cdk/aws-dynamodb. So, let&#39;s\n    add it by executing npm install @aws-cdk/aws-dynamodb@1.19.0.\n\n\nimport {Code, Function, FunctionProps, Runtime} from &quot;@aws-cdk/aws-lambda&quot;;\nimport {Construct, Stack, StackProps} from &quot;@aws-cdk/core&quot;;\nimport {AttributeType, Table, TableProps} from &quot;@aws-cdk/aws-dynamodb&quot;;\n\nexport class OrderServiceInfraStack extends Stack {\n    constructor(scope: Construct, id: string, props?: StackProps) {\n        super(scope, id, props);\n\n        const functionProperties: FunctionProps = {\n            code: Code.fromAsset(&quot;../dist&quot;),\n            handler: &quot;handler.ordersHandler&quot;,\n            runtime: Runtime.NODEJS_10_X,\n            functionName: &quot;order-service-function&quot;,\n            environment: {&quot;ExecutionEnvironment&quot;: &quot;dev&quot;}\n        };\n        new Function(this, &quot;order-service-function&quot;, functionProperties);\n\n        //create TableProps\n        const tableProps: TableProps = {\n            partitionKey: {\n                name: &quot;orderId&quot;,\n                type: AttributeType.STRING\n            },\n            tableName: &quot;orders&quot;\n        };\n        //create a dynamo table in the stack\n        new Table(this, &quot;order-table&quot;, tableProps);\n    }\n}\n\n\nFew quick observations - \n\n\n\n    We have imported Table class from @aws-cdk/aws-dynamodb\n    Constructor of Table class takes 3 parameters -\n        \n            scope: Construct - which identifies the parent resource\n            id: string - unique identifier of the resource within the stack\n            props: TableProps - table properties including name of the table, partitionKey etc\n        \n    \n\n\n\nThat&#39;s it. Our dynamo table resource is created in the stack. Let&#39;s verify by writing a unit test.\n\n\ntest(&quot;stack should contain a dynamodb table with table name&quot;, () =&amp;gt; {\n    const app = new App();\n    const stack = new OrderServiceInfraStack(app, &quot;OrderServiceStack&quot;);\n\n    //assert that stack contains a dynamo table with &quot;orders&quot; as the table name\n    expect(stack).toHaveResource(&quot;AWS::DynamoDB::Table&quot;, {\n        &quot;TableName&quot;: &quot;orders&quot;\n    })\n});\n\ntest(&quot;stack should contain a dynamodb table with orderId as the Hash key&quot;, () =&amp;gt; {\n    const app = new App();\n    const stack = new OrderServiceInfraStack(app, &quot;OrderServiceStack&quot;);\n\n    //assert that stack contains a dynamo table with &quot;orderId&quot; as the HASH key\n    expect(stack).toHaveResource(&quot;AWS::DynamoDB::Table&quot;, {\n        &quot;KeySchema&quot;: [\n            {\n                &quot;AttributeName&quot;: &quot;orderId&quot;,\n                &quot;KeyType&quot;: &quot;HASH&quot;\n            }\n        ]\n    })\n});\n\n\nStep 5: Refactoring the stack\n\n\nLet&#39;s look at a unit test and see if there are any challenges in understanding it.\n\n\ntest(&quot;stack should contain a lambda function with node10 as runtime&quot;, () =&amp;gt; {\n    const app = new App();\n    const stack = new OrderServiceInfraStack(app, &quot;OrderServiceStack&quot;);\n\n    expect(stack).toHaveResource(&quot;AWS::Lambda::Function&quot;, {\n        Runtime: Runtime.NODEJS_10_X.toString()\n    })\n});\n\n\n\n    How do you know our stack will contain a lambda function with node10 as the runtime? Honestly, there is no\n        relation between the test input and its output\n    \n    Even though the test is for OrderServiceStack, I see that we are trying to assert on properties of a resource.\n        It somehow looks to me like a misplaced test\n    \n\n\n\nIn order to solve both the problems, we can create a component (or a class) which accepts configuration properties and\n    creates a lambda function. This means we will be able to move lambda function unit tests closer to that class and\n    make the unit tests more revealing. Let&#39;s see how. \n\n\nLet&#39;s consider that all our lambda functions are based on &quot;node10&quot; runtime. With this consideration, we can create a\n    class, Node10LambdaFunction that represents a lambda function and accepts Node10LambdaFunctionProperties.\nThis is the way which we will take in the article, you should try other approaches and please share them.\nimport {Code, Function, FunctionProps, Runtime} from &quot;@aws-cdk/aws-lambda&quot;;\nimport {Construct} from &quot;@aws-cdk/core&quot;;\n\n//inherit from Function\nexport class Node10LambdaFunction extends Function {\n    //accepts Node10FunctionProperties which will contain attributes that make sense for our project\n    constructor(scope: Construct, properties: Node10FunctionProperties) {\n        super(scope, properties.functionName, properties.toFunctionProps())\n    }\n}\n\nclass Node10LambdaFunctionProperties {\n    //attributes that make sense at this stage\n    constructor(private readonly code: Code,\n                private readonly handler: string,\n                readonly functionName: string,\n                private readonly environmentVariables?: {[key: string]: string }) {\n    }\n\n    //behavior to return AWS FunctionProps\n    toFunctionProps(): FunctionProps {\n        return {\n            code: this.code,\n            handler: this.handler,\n            runtime: Runtime.NODEJS_10_X,\n            functionName: this.functionName,\n            environment: this.environmentVariables\n        }\n    }\n}\n\n\nFew quick observations - \n\n\n\n    Client of Node10LambdaFunction (which is going be our stack now) is not required to pass runtime as it is\n        evident from the name itself\n    \n    Client code is not required to pass id of the resource. Node10LambdaFunction passes function name as the id of\n        the resource\n    \n\n\n\nNow, we can move the lambda function unit tests closer to Node10LambdaFunction. This is how the updated\n    test(s) will look like -\n\n\ntest(&quot;stack should contain a lambda function with node10 as runtime&quot;, () =&amp;gt; {\n    const stack = new Stack();\n    const properties = new Node10LambdaFunctionProperties(\n        Code.fromAsset(&quot;../dist&quot;),\n        &quot;handler.ordersHandler&quot;,\n        &quot;order-service-function&quot;);\n\n    //name of the class indicates a lambda function with node10 as the runtime will be created\n    new Node10LambdaFunction(stack, properties);\n\n    //assert that stack contains a lambda function with node10 as the runtime. This time the test is not magical\n    expect(stack).toHaveResource(&quot;AWS::Lambda::Function&quot;, {\n        Runtime: Runtime.NODEJS_10_X.toString()\n    })\n});\n\ntest(&quot;stack should contain a lambda function with specified environment variable&quot;, () =&amp;gt; {\n    const stack = new Stack();\n    const properties = new Node10LambdaFunctionProperties(\n        Code.fromAsset(&quot;../dist&quot;),\n        &quot;handler.ordersHandler&quot;,\n        &quot;order-service-function&quot;,\n        {&quot;env&quot;: &quot;dev&quot;});\n\n    new Node10LambdaFunction(stack, properties);\n\n    //assert that stack contains a lambda function with provided environment variable\n    expect(stack).toHaveResource(&quot;AWS::Lambda::Function&quot;, {\n        Environment: {\n            Variables: {\n                &quot;env&quot;: &quot;dev&quot;\n            }\n        }\n    })\n});\n\n\nHere, we are not instantiating OrderServiceStack but creating an empty stack which gets passed to\n    Node10LambdaFunction. \n\n\nSimilarly, we can write other tests around lambda function like - assert that lambda function is created with a given\n    name, assert that lambda function is inside a VPC etc. I will make similar changes for Dynamo table and commit the\n    code.\n\n\nWith these changes, we can write unit tests for various components (as fine grained as we want) and a snapshot test\n    for the entire stack.\n\n\nStep 6: Adding lambda backed public RestApi to stack\n\n\nLet&#39;s provision a rest api. In order to do so we need to add a dependency @aws-cdk/aws-apigateway. So,\n    let&#39;s add it by executing npm install @aws-cdk/aws-apigateway@1.19.0.\n\n\nFollowing the same pattern we would like to create a class that allows us to add an endpoint which can be accessed\n    publicly and is backed by a lambda function.\n\n\nimport {LambdaRestApi, LambdaRestApiProps, MethodLoggingLevel} from &quot;@aws-cdk/aws-apigateway&quot;;\nimport {Construct} from &quot;@aws-cdk/core&quot;;\nimport {Node10LambdaFunction} from &quot;../../function/Node10LambdaFunction&quot;;\nimport {IFunction} from &quot;@aws-cdk/aws-lambda&quot;;\n\n//inherit from LambdaRestApi\nclass LambdaBackedPublicRestApi extends LambdaRestApi {\n\n    //similar to Node10Function, it accepts LambdaBackedPublicRestApiProperties\n    constructor(scope: Construct, properties: LambdaBackedPublicRestApiProperties) {\n        super(scope, properties.apiName, properties.toLambdaRestApiProps());\n    }\n}\n\nclass LambdaBackedPublicRestApiProperties {\n\n    constructor(readonly apiName: string,\n                private readonly stageName: string,\n                private handler: Node10LambdaFunction) {\n    }\n\n    //behavior to return LambdaRestApiProps\n    toLambdaRestApiProps(): LambdaRestApiProps {\n        return {\n            restApiName: this.apiName,\n            deployOptions: {\n                stageName: this.stageName,\n                loggingLevel: MethodLoggingLevel.INFO\n            },\n            proxy: false,\n            handler: this.handler as IFunction\n        }\n    }\n}\n\n\nThis will create a RestApi in the stack for us but there is no endpoint available for us. In order to allow that to\n    happen we can expose a method that takes a resource path say - &quot;orders/{orderId}&quot; and an http method\n    which needs to be attached to the last part of resource which in this example is {orderId}.\n\n\nSo, let&#39;s do this.\n\n\nimport {\n    LambdaRestApi,\n    LambdaRestApiProps,\n    MethodLoggingLevel,\n    Resource\n} from &quot;@aws-cdk/aws-apigateway&quot;;\nimport {Construct} from &quot;@aws-cdk/core&quot;;\nimport {Node10LambdaFunction} from &quot;../../function/Node10LambdaFunction&quot;;\nimport {IFunction} from &quot;@aws-cdk/aws-lambda&quot;;\n\nclass LambdaBackedPublicRestApi extends LambdaRestApi {\n\n    constructor(scope: Construct, properties: LambdaBackedPublicRestApiProperties) {\n        super(scope, properties.apiName, properties.toLambdaRestApiProps());\n    }\n\n    //add resource say, orders/{orderId} and a method GET against {orderId}\n    addEndpoint(resourcePath: string, httpMethod: HttpMethod) {\n        if (resourcePath.startsWith(&quot;/&quot;))\n            throw new IllegalArgumentException(\n                `${resourcePath} should not begin with a / while adding a rest endpoint`\n            );\n\n        const resource = this.addAllResourcesUsing(resourcePath);\n        resource.addMethod(httpMethod);\n    }\n\n    //add resources recursively\n    private addAllResourcesUsing(resourcePath: string): Resource {\n        function add(resources: string[], rootResource: Resource): Resource {\n            if (resources.length === 0)\n                return rootResource;\n            else\n                return add(\n                    resources.slice(1, resources.length),\n                    LambdaBackedPublicRestApi.getOrAdd(resources[0], rootResource)\n                );\n        }\n\n        return add(resourcePath.split(&quot;/&quot;), (this.root as Resource));\n    }\n\n    //return the already added resource or add\n    private static getOrAdd(resourcePath: string, rootResource: Resource): Resource {\n        const alreadyPresentResource = rootResource.getResource(resourcePath) as Resource;\n        return alreadyPresentResource || rootResource.addResource(resourcePath)\n    }\n}\n\nenum HttpMethod {\n    GET = &quot;GET&quot;\n}\n\nclass IllegalArgumentException extends Error {\n}\n\n\nFew quick observations - \n\n\n\n    We do not expect the resource path to begin with a &quot;/&quot;, aws-apigateway throws an error if that is\n        the case\n    \n    We are recursively adding each resource from the resource path\n    Http method gets added on the last resource of the resource path\n    getOrAdd ensures that we do not add the same resource again. Eg; if we want to add 2 resource paths serverless/lambda and serverless/lambda/{functionId}, it is necessary to ensure we do not add serverless/lambda again\n\n\n\nLet&#39;s quickly add a couple of unit tests.\n\n\nimport {LambdaBackedPublicRestApi} from &quot;../../../lib/restapi/public/LambdaBackedPublicRestApi&quot;;\nimport {Stack} from &quot;@aws-cdk/core&quot;;\nimport {LambdaBackedPublicRestApiProperties} from &quot;../../../lib/restapi/public/LambdaBackedPublicRestApiProperties&quot;;\nimport {Node10LambdaFunctionProperties} from &quot;../../../lib/function/Node10LambdaFunctionProperties&quot;;\nimport {Code} from &quot;@aws-cdk/aws-lambda&quot;;\nimport {Node10LambdaFunction} from &quot;../../../lib/function/Node10LambdaFunction&quot;;\nimport {HttpMethod} from &quot;../../../lib/restapi/public/HttpMethod&quot;;\nimport {CfnMethod} from &quot;@aws-cdk/aws-apigateway&quot;;\nimport &quot;@aws-cdk/assert/jest&quot;;\n\nconst addFakeEndpoint = (api: LambdaBackedPublicRestApi) =&amp;gt; {\n    api.addEndpoint(&quot;fake&quot;, HttpMethod.GET);\n};\n\ntest(&quot;stack should contain a public api with a name&quot;, () =&amp;gt; {\n    const stack = new Stack();\n    const node10LambdaFunction = new Node10LambdaFunction(\n        stack,\n        new Node10LambdaFunctionProperties(\n            Code.fromAsset(&quot;../dist&quot;),\n            &quot;handler.ordersHandler&quot;,\n            &quot;order-service-function&quot;));\n\n    const properties = new LambdaBackedPublicRestApiProperties(\n        &quot;orders-api&quot;,\n        &quot;dev&quot;,\n        node10LambdaFunction\n    );\n\n    const api = new LambdaBackedPublicRestApi(stack, properties);\n    addFakeEndpoint(api);\n\n    //assert that stack contains a rest api with &quot;orders-api&quot; as the name\n    expect(stack).toHaveResource(&quot;AWS::ApiGateway::RestApi&quot;, {\n        Name: &quot;orders-api&quot;\n    });\n});\n\ntest(&quot;stack should contain a public api with an http method GET added to the resource&quot;, () =&amp;gt; {\n    const stack = new Stack();\n    const node10LambdaFunction = new Node10LambdaFunction(\n        stack,\n        new Node10LambdaFunctionProperties(\n            Code.fromAsset(&quot;../dist&quot;),\n            &quot;handler.ordersHandler&quot;,\n            &quot;order-service-function&quot;));\n\n    const properties = new LambdaBackedPublicRestApiProperties(\n        &quot;orders-api&quot;,\n        &quot;dev&quot;,\n        node10LambdaFunction\n    );\n\n    const api = new LambdaBackedPublicRestApi(stack, properties);\n    api.addEndpoint(&quot;article/serverless&quot;, HttpMethod.GET);\n\n    //get a resource and a CfnMethod against that resource\n    const serverlessResource = api.root.getResource(&quot;article&quot;)?.getResource(&quot;serverless&quot;);\n    const method = serverlessResource?.node.findChild(&quot;GET&quot;) as CfnMethod;\n\n    expect(method.httpMethod).toEqual(HttpMethod.GET);\n});\n\n\nThese tests assert that a rest api exists with a given name and an http method is attached to a resource.\n\n\nStep 7: Updating the stack\n\n\nLet&#39;s update the stack to have lambda function, dynamo table, lambda backed public api and dynamo table read access\n    to lambda function.\n\n\nimport {Code} from &quot;@aws-cdk/aws-lambda&quot;;\nimport {Construct, Stack, StackProps} from &quot;@aws-cdk/core&quot;;\nimport {AttributeType} from &quot;@aws-cdk/aws-dynamodb&quot;;\nimport {Node10LambdaFunction} from &quot;./function/Node10LambdaFunction&quot;;\nimport {Node10LambdaFunctionProperties} from &quot;./function/Node10LambdaFunctionProperties&quot;;\nimport {DynamoTable} from &quot;./dynamodb/DynamoTable&quot;;\nimport {DynamoTableProperties} from &quot;./dynamodb/DynamoTableProperties&quot;;\nimport {PrimaryKey} from &quot;./dynamodb/PrimaryKey&quot;;\nimport {PartitionKey} from &quot;./dynamodb/PartitionKey&quot;;\nimport {LambdaBackedPublicRestApi} from &quot;./restapi/public/LambdaBackedPublicRestApi&quot;;\nimport {LambdaBackedPublicRestApiProperties} from &quot;./restapi/public/LambdaBackedPublicRestApiProperties&quot;;\nimport {HttpMethod} from &quot;./restapi/public/HttpMethod&quot;;\n\nexport class OrderServiceInfraStack extends Stack {\n    constructor(scope: Construct, id: string, props?: StackProps) {\n        super(scope, id, props);\n\n        //use the newly prepared classes\n        const ordersFunction = this.ordersFunction();\n        const ordersTable    = this.ordersTable();\n        const restApi        = this.lambdaBackedPublicRestApi(ordersFunction);\n\n        restApi.addEndpoint(&quot;orders/{orderId}&quot;, HttpMethod.GET); //add the required resources along with HTTP method\n        ordersTable.grantReadData(ordersFunction); //grant read access on &quot;orders&quot; table to lambda function\n    }\n\n    //returns an instance of Node10LambdaFunction\n    private ordersFunction() {\n        return new Node10LambdaFunction(this, new Node10LambdaFunctionProperties(\n            Code.fromAsset(&quot;../dist&quot;),\n            &quot;handler.ordersHandler&quot;,\n            &quot;order-service-function&quot;,\n            {&quot;ExecutionEnvironment&quot;: &quot;dev&quot;})\n        );\n    }\n\n    //returns an instance of DynamoTable\n    private ordersTable() {\n        return new DynamoTable(this, new DynamoTableProperties(\n            &quot;orders&quot;,\n            new PrimaryKey(\n                new PartitionKey(\n                    &quot;orderId&quot;,\n                    AttributeType.STRING)\n            ))\n        );\n    }\n\n    //returns an instance of LambdaBackedPublicRestApi\n    private lambdaBackedPublicRestApi(lambda: Node10LambdaFunction) {\n        return new LambdaBackedPublicRestApi(this, new LambdaBackedPublicRestApiProperties(\n            &quot;orders-api&quot;,\n            &quot;dev&quot;,\n            lambda\n        ));\n    }\n}\n\n\nTime to add our snapshot test, probably simpler than you might have thought of -\n\n\nimport {OrderServiceInfraStack} from &quot;../lib/OrderServiceInfraStack&quot;;\nimport {App} from &quot;@aws-cdk/core&quot;;\nimport &quot;@aws-cdk/assert/jest&quot;;\n\ntest(&quot;should create order service stack&quot;, () =&amp;gt; {\n    const app = new App();\n    const stack = new OrderServiceInfraStack(app, &quot;OrderServiceStack&quot;);\n\n    expect(stack).toMatchSnapshot();\n});\n\n\nStep 8: Deploying our stack\n\n\nWe have worked hard to create all the resources that are needed in our stack. Now is the time to deploy our stack and\n    see things in action.\n\n\nLet&#39;s update OrderServiceInfra to pass stack name as a part of stack properties. It is this file which\n    acts as an entry point for the application and is referred in cdk.json.\n\n\n#!/usr/bin/env node\nimport &quot;source-map-support/register&quot;;\nimport {OrderServiceInfraStack} from &quot;../lib/OrderServiceInfraStack&quot;;\nimport {StackProps} from &quot;@aws-cdk/core&quot;;\nimport cdk = require(&quot;@aws-cdk/core&quot;);\n\nconst app = new cdk.App();\n//pass stack name\nconst stackProps:StackProps = {\n    stackName: &quot;order-service-stack&quot;\n};\n//instantiate OrderServiceInfraStack\nnew OrderServiceInfraStack(app, &quot;OrderServiceStack&quot;, stackProps);\n\n\nCDK also provides us with various commands - \n\n\n\n    cdk list - lists the stacks\n    cdk deploy - deploys the stack in AWS environment\n    cdk destroy - destroys the stacks\n    cdk synthesize - synthesizes and prints the CloudFormation\n    cdk bootstrap - deploys the CDK toolkit stack into an AWS environment\n\n\n\nWe need to execute cdk bootrap and cdk deploy from infra directory to deploy\n    stack in our AWS account.\n\n\n&amp;gt; cd infra\n&amp;gt; cdk bootstrap\n&amp;gt; cdk deploy\n\n\nThese commands make a few assumptions -\n\n\n\n    AWS credentials are already configured on host machine\n    AWS user has the right to create various AWS resources including IAM roles\n    dist/ directory which will be deployed on an S3 bucket (bootstrap creates for us) when we execute\n        cdk bootstrap, already exists\n    \n\n\n\nIt will take sometime for stack to be created which will consist of lambda function, dynamo table, api gateway\n    and all the necessary IAM roles.\n\n\nOnce our stack is created, make an entry in orders table, hit the public api endpoint which will look\n    like https://rest-api-id.execute-api.ap-south-1.amazonaws.com/dev/orders/OrderId and enjoy the\n    output. \n\n\nThat&#39;s it, our stack is deployed and our application is up and running 😁\n\n\nConclusion \n\n\nRelationship between CDK and CloudFormation can be summarised as -\n\n\n\n    \n\n\n\nIn this article we were able to code our infra using CDK, write tests for our infra and deploy the same. Let&#39;s take a\n    look at some of the advantages of using CDK -\n\n\n\n\n    Resources can be modeled in Object Oriented manner\n    High level abstractions can be defined and published within the team or company\n    Infrastructure can be built as library\n    Infrastructure code can be tested\n    IDE&#39;s code completion can be leveraged\n    Programming language constructs like if statements, for-loops, etc can be used when defining infrastructure\n\n\n\nWe have finally come to end of our Serverless Journey series. Hope you enjoyed it.\n\n"
} ,
  
  {
    "title"    : "Testing Serverless Journey",
    "category" : "",
    "tags"     : " AWS Lambda, Serverless",
    "url"      : "/testing-serverless-journey/",
    "date"     : "March 14, 2020",
    "excerpt"  : "\nIt is time to test our Serverless journey which started with a web application that involved AWS lambda, AWS API Gateway and AWS DynamoDB. \n\n\nWe had some unit tests for our controller, service and request objects. But, these tests don&#39;t give us t...",
  "content"  : "\nIt is time to test our Serverless journey which started with a web application that involved AWS lambda, AWS API Gateway and AWS DynamoDB. \n\n\nWe had some unit tests for our controller, service and request objects. But, these tests don&#39;t give us the kind of confidence we need to deploy our application. At this stage we don&#39;t even know if the query that is written in repository is going to work properly or not, forget about releasing the application.\n\n\nWhat we need is an ability to test the following - \n\n\n\nRepository works as expected by connecting to DynamoDB\nLambda handler is able to receive an event from API Gateway and get an order by its id\n\n\n\nIn simple terms we need some form of integration testing. \n\n\nLet&#39;s welcome LocalStack\n\n\nLocalStack is a fully functional local AWS cloud stack. Its github page states the following - \n\n\nLocalStack&amp;nbsp;provides an easy-to-use test/mocking framework for developing Cloud applications. Currently, the focus is primarily on supporting the AWS cloud stack.  \n\n\n\nLocalStack spins up various Cloud APIs on local machine including S3, Lambda, DynamoDB and API Gateway. This is all we need to test our complete application. \n\n\nSome of you might be having a question &quot;Why is S3 needed?&quot;. Well, we will get an answer to this by the end of this article. So, please hold on.\n\n\nUnderstanding LocalStack\n\n\nLocalStack can be made to run as a docker container on a host machine. It supports quite a number of AWS services which will run inside the docker container with different ports exposed on host machine. \n\n\nBefore moving on let&#39;s look at this conversation to understand how can LocalStack be leveraged for testing Serverless application. \n\n\n    \n\n\n\n\nLet&#39;s take some small steps to test our application using LocalStack.\n\n\nWriting Repository Test\n\n\nIn order to test repository layer we need to -\n\n\n\nBuild the project\nHave a docker container with a running DynamoDB service\nFacilitate creation of &quot;orders&quot; table in DynamoDB service\nChange the application to connect to local DynamoDB service\nAdd integration tests for repository\n\n\n\nLet&#39;s handle each of them one by one.\n\n\nStep 1: Build the project\n\n\nLet&#39;s add a build task to our outer package.json which will execute tsc. Let&#39;s also add a types definition for node by executing npm i @types/node. Here, is how our build script looks like - \n\n\n&quot;scripts&quot;: {\n  &quot;test&quot;: &quot;jest test/**&quot;,\n  &quot;build&quot;: &quot;tsc&quot;\n}\n\n\nBefore we execute our build command, let&#39;s exclude infra and test folders from our outer tsconfig.json.\n\n\n{\n  &quot;compilerOptions&quot;: {\n    &quot;noEmitOnError&quot;: true,\n    &quot;moduleResolution&quot;: &quot;node&quot;,\n    &quot;module&quot;: &quot;commonjs&quot;,\n    &quot;target&quot;: &quot;es6&quot;,\n    &quot;outDir&quot;: &quot;dist&quot;,\n    &quot;inlineSourceMap&quot;: true\n  },\n  &quot;exclude&quot;: [&quot;infra&quot;, &quot;test&quot;]\n}\n\n\nNow, we can execute npm run build which should produce a dist folder with compiled code.\n\n\nNote we are using tsc to transpile our typescript code to javascript. We do not have any external dependencies to be packed with our distribution, had there been any we would have gone ahead with webpack.\n\n\nStep 2: Docker container with a running DynamoDB service\n\n\nLet&#39;s create a docker-compose.yml file referring to LocalStack image and start the container as a pretest step in our package.json.\n\n\nversion: &quot;2.1&quot;\nservices:\n  localstack:\n    image: localstack/localstack:0.10.7 ## use localstack image\n    ports:\n      - &quot;4567-4599:4567-4599&quot;\n      - &quot;${PORT_WEB_UI-8080}:${PORT_WEB_UI-8080}&quot;\n    environment:\n      - SERVICES=${SERVICES- }\n      - DEBUG=1\n      - DATA_DIR=${DATA_DIR- }\n      - PORT_WEB_UI=${PORT_WEB_UI- }\n      - LAMBDA_EXECUTOR=${LAMBDA_EXECUTOR- }\n    volumes:\n      - &quot;/var/run/docker.sock:/var/run/docker.sock&quot;\n &quot;scripts&quot;: {\n    &quot;pretest&quot;: &quot;docker-compose -f test/docker-compose.yml up -d&quot;, //start docker compose before running the tests\n    &quot;test&quot;: &quot;jest test/**&quot;,\n    &quot;build&quot;: &quot;tsc&quot;\n}\n\n\nRun the pretest command and see LocalStack running as docker container.\n\n\nStep 3: Facilitate creation of &quot;orders&quot; table in DynamoDB service\n\n\nWith LocalStack container up and running, &quot;orders&quot; table needs to be created in DynamoDB service. In order to do this we will use CloudFormation template. So, let&#39;s write one -\n\n\nAWSTemplateFormatVersion: &quot;2010-09-09&quot;\nResources:\n  OrdersTable:\n    Type: AWS::DynamoDB::Table\n    Properties:\n      TableName: &quot;orders&quot; ## create &quot;orders&quot; table\n      AttributeDefinitions:\n        - AttributeName: &quot;orderId&quot;\n          AttributeType: &quot;S&quot; ## use STRING as the data type for &quot;orderId&quot;\n      KeySchema:\n        - AttributeName: &quot;orderId&quot; ## use &quot;orderId&quot; as the HASH key\n          KeyType: &quot;HASH&quot;\n\n\nWe will also create a script to deploy CloudFormation template against LocalStack. This script will also be executed as a part of our pretest step.\n\n\n#!/bin/sh\n\naws cloudformation deploy \\\n--template-file template.yaml \\\n--stack-name order-service-stack \\\n--region us-east-1 \\\n--capabilities CAPABILITY_IAM  \\\n--endpoint-url http://localhost:4581\n\necho &#39;aws cloudformation deploy executed against localstack&#39;\n\n\nFew Quick Observations - \n\n\n\nAs a part of &quot;aws cloudformation deploy&quot;, us-east-1 has been specified as the region. By default, LocalStack runs with us-east-1 and we are using the same region\nAs a part of &quot;aws cloudformation deploy&quot;, we use 4581 as the port for local CloudFormation service which is exposed by LocalStack \n\n\n\nLet&#39;s update our package.json.\n\n\n &quot;scripts&quot;: {\n    &quot;localstack:up&quot;: &quot;docker-compose -f test/docker-compose.yml up -d&quot;,\n    &quot;delay&quot;: &quot;sleep 20&quot;,\n    &quot;localstack:create-infra&quot;: &quot;cd test/infra &amp;amp;&amp;amp; ./deploy.sh&quot;,\n    //start docker compose, introduce some delay and run the above script as a part of pretest\n    &quot;pretest&quot;: &quot;npm run localstack:up &amp;amp;&amp;amp; npm run delay &amp;amp;&amp;amp; npm run localstack:create-infra&quot;,\n    &quot;test&quot;: &quot;jest test/**&quot;,\n    &quot;build&quot;: &quot;tsc&quot;\n}\n\n\nFollowing events happens as a part of pretest step -\n\n\n\nLocalStack docker container starts\nSome delay gets introduced to allow localstack services to be available\nCloudFormation template gets deployed against LocalStack by running deploy.sh\n\n\n\nBefore CloudFormation template can be deployed on LocalStack, a small delay has been specified to ensure LocalStack with its services is up and running.\n\n\nHow do I know if &quot;orders&quot; table was created?\n\n\nLocalStack tries to replicate AWS services on local. By this theory, we should be able to run AWS commands by specifying the endpoint-url of the corresponding service.\n\n\n &amp;gt; aws dynamodb scan --table-name orders --endpoint-url http://localhost:4569\n\n//Output\n{\n    &quot;Count&quot;: 0,\n    &quot;Items&quot;: [],\n    &quot;ScannedCount&quot;: 0,\n    &quot;ConsumedCapacity&quot;: null\n}\n\n\nNow, the last step is making a change in the application to connect to local DynamoDB.\n\n\nStep 4: Connecting the application to local DynamoDB service\n\n\nLet&#39;s change the repository layer to connect to local DynamoDB service.\n\n\nimport {GetItemInput} from &quot;aws-sdk/clients/dynamodb&quot;;\nimport {Order} from &quot;../model/Order&quot;;\nimport {dynamoDbClient} from &quot;../DynamoDbConfiguration&quot;;\n\nconst dynamoDb = dynamoDbClient(); //get dynamoDbClient from DynamoDbConfiguration\n\nexport class OrderRepository {\n\n    async findAnOrderBy(id: string) {\n        const getItemInputOptions: GetItemInput = {\n            TableName: &quot;orders&quot;, //table name\n            Key: {\n                &quot;orderId&quot;: {S: id} //query against orderId attribute of order item\n            }\n        };\n        const response = await dynamoDb.getItem(getItemInputOptions).promise(); //get a dynamo item by passing its id\n        return response.Item ? Order.from(response.Item) : null; //map dynamo item to Order\n    }\n}\n\n\nDynamoDbConfiguration looks like - \n\n\nimport {DynamoDB} from &quot;aws-sdk&quot;\n\nconst executionEnvironment = () =&amp;gt; {\n    const defaultExecutionEnvironment = &quot;test&quot;;\n    //accept ExecutionEnvironment as the lambda environment variable\n    return process.env.ExecutionEnvironment || defaultExecutionEnvironment;\n};\n\nconst isTestExecutionEnvironment = () =&amp;gt; executionEnvironment() === &quot;test&quot;;\n\nexport const dynamoDbClient = () =&amp;gt; {\n    if (isTestExecutionEnvironment()) {\n        //return an instance of DynamoDB connecting to local dynamo endpoint exposed by localstack, if the execution environment is &quot;test&quot;\n        return new DynamoDB({\n            &quot;region&quot;: &quot;us-east-1&quot;,\n            &quot;endpoint&quot;: &quot;http://localhost:4569&quot;\n        });\n    } else {\n        //return an instance of DynamoDB connecting to the actual region in AWS\n        return new DynamoDB({\n            &quot;region&quot;: &quot;ap-south-1&quot;\n        });\n    }\n};\n\n\nFew Quick Observations - \n\n\n\nOrderRepository uses dynamoDbClient exposed by DynamoDbConfiguration globally. The reason being &quot;aws-sdk&quot; needs to be initialised during cold startup of lambda function\nDynamoDbConfiguration uses a lambda environment variable to determine if the execution environment is &quot;test&quot;. By default, execution environment is considered as &quot;test&quot;\nIf execution environment is &quot;test&quot;, then an instance of DynamoDB connecting to local dynamo service is returned\n    This also means ExecutionEnvironment needs to be passed during deployment as lambda environment variable\n\n\n\nStep 5: Adding Integration Tests for Repository\n\n\nimport {DeleteItemInput, PutItemInput} from &quot;aws-sdk/clients/dynamodb&quot;;\n\nimport {OrderRepository} from &quot;../src/repository/OrderRepository&quot;;\nimport {Order}           from &quot;../src/model/Order&quot;;\nimport {dynamoDbClient}  from &quot;../src/DynamoDbConfiguration&quot;;\n\nconst dynamoDb = dynamoDbClient();\n\ntest(&quot;should return an order given there is AN order for the provided order id&quot;, async () =&amp;gt; {\n    const orderId = &quot;order-100&quot;;\n    \n    await OrderRepositoryFixture.deleteAnOrder(orderId); //delete an existing order\n    await OrderRepositoryFixture.createAn(new Order(orderId, 5000)); //save an order in dynamo table\n    \n    const order = await new OrderRepository().findAnOrderBy(orderId); //find an order by orderId\n\n    expect(order.orderId).toEqual(orderId);\n    expect(order.amount).toEqual(5000);\n});\n\ntest(&quot;should NOT return an order given there is NO order for the provided order id&quot;, async () =&amp;gt; {\n    const orderId = &quot;no-order-present-for-this-order-id&quot;;\n    const order = await new OrderRepository().findAnOrderBy(orderId);\n\n    expect(order).toBeNull();\n});\n\nclass OrderRepositoryFixture {\n    static async createAn(order: Order) {\n        const item: PutItemInput = {\n            TableName: &quot;orders&quot;, //table name\n            Item: {\n                &quot;orderId&quot;: {\n                    S: order.orderId //STRING orderId\n                },\n                &quot;amount&quot;: {\n                    N: order.amount.toString() //NUMERIC amount\n                }\n            }\n        };\n        await dynamoDb.putItem(item).promise(); //save the order\n    }\n    static async deleteAnOrder(orderId: string) {\n        const item: DeleteItemInput = {\n            TableName: &quot;orders&quot;,\n            Key: {\n                &quot;orderId&quot;: {\n                    S: orderId\n                }\n            }\n        };\n        await dynamoDb.deleteItem(item).promise(); //delete the order\n    }\n}\n\n\nThat&#39;s it run all the tests npm t and see them pass 😁\n\n\nConnecting the dots\n\n\nWe have been able test our repository against DynamoDB service running in LocalStack. What we want to do next is - \n\n\n\nDeploy lambda function code against lambda function service in LocalStack\nCreate Rest Api backed by lambda function in LocalStack\nAdding integration tests to send an http request against the Api Gateway\n\n\n\nLet&#39;s start.\n\n\nStep 6: Deploy lambda function code\n\n\nIn order to deploy the lambda function code, we need to build the code, archive it, upload the archive on S3 service running inside LocalStack and update CloudFormation template to create the lambda function by referring to the S3 bucket.\n\n\nLet&#39;s archive the code, create an S3 bucket and upload the archive on S3 service.\n\n\n&quot;scripts&quot;: {\n    &quot;localstack:down&quot;: &quot;docker-compose -f test/docker-compose.yml down&quot;,\n    &quot;localstack:up&quot;: &quot;docker-compose -f test/docker-compose.yml up -d&quot;,\n    &quot;delay&quot;: &quot;sleep 20&quot;,\n    &quot;localstack:create-infra&quot;: &quot;cd test/infra &amp;amp;&amp;amp; ./init.sh&quot;,\n    &quot;archive&quot;: &quot;cd dist/ &amp;amp;&amp;amp; zip -r ../serverless-order-service.zip .&quot;,\n    &quot;pretest&quot;: &quot;npm run build &amp;amp;&amp;amp; npm run archive &amp;amp;&amp;amp; npm run localstack:down &amp;amp;&amp;amp; npm run localstack:up &amp;amp;&amp;amp; npm run delay &amp;amp;&amp;amp; npm run localstack:create-infra&quot;,\n    &quot;test&quot;: &quot;jest test/**&quot;,\n    &quot;build&quot;: &quot;tsc&quot;\n}\n\n\nFollowing events happens as a part of pretest step –\n\n\n\nCode gets built using npm run build\nDistribution gets archived using npm run archive\nLocalStack docker container gets stopped using npm run localstack:down\nLocalStack docker container starts using npm run localstack:up\nSome delay gets introduced using npm run delay\nCloudFormation template gets deployed against LocalStack using npm run localstack:create-infra\n\n\n\npackage.json now runs build and archive as a part of pretest step. archive simply creates a zip file of the built code which is ready to be uploaded on an S3 bucket. localstack:create-infra now runs init.sh which delegates the job of creating a bucket, uploading the archive and deploying the infra to different scripts (we shall see it soon).\n\n\nLet&#39;s see the commands to create an S3 bucket and upload the archive. These are the same commands which will be executed from one of our shell scripts -\n\n\n&amp;gt; aws s3 mb s3://serverless-order-service --endpoint-url http://localhost:4572\n\n&amp;gt; aws s3 cp ../../serverless-order-service.zip \\\ns3://serverless-order-service --endpoint-url http://localhost:4572\n\n\nLet&#39;s update CloudFormation template to create lambda resource.\n\n\nAWSTemplateFormatVersion: &quot;2010-09-09&quot;\nResources:\n  OrdersFunction:\n    Type: AWS::Lambda::Function\n    Properties:\n      FunctionName: &quot;order-service-function&quot;\n      Runtime: &quot;nodejs10.x&quot;\n      Handler: &quot;handler.ordersHandler&quot;\n      Code:\n        S3Bucket: &quot;serverless-order-service&quot;   ## created earlier\n        S3Key: &quot;serverless-order-service.zip&quot;  ## uploaded earlier\n        Role: !GetAtt &quot;OrdersRole.Arn&quot;         ## refer to a dummy role created below\n\n  OrdersRole:\n    Type: AWS::IAM::Role                       ## dummy role\n    Properties:\n      RoleName: &quot;OrdersFunctionRole&quot;\n      AssumeRolePolicyDocument:\n        Version: &quot;2012-10-17&quot;\n        Statement:\n          - Effect: &quot;Allow&quot;\n            Action:\n              - &quot;sts:AssumeRole&quot;\n            Principal:\n              Service:\n                - &quot;lambda.amazonaws.com&quot;\n\n\nExecuting npm t should now create AWS lambda function in LocalStack. We can verify the same -\n\n\n&amp;gt; aws lambda list-functions --endpoint-url http://localhost:4574/\n\n//Output\n{\n    &quot;FunctionArn&quot;: &quot;arn:aws:lambda:us-east-1:000000000000:function:order-service-function&quot;, \n    &quot;Handler&quot;: &quot;handler.ordersHandler&quot;, \n    &quot;Role&quot;: &quot;test_role&quot;, \n    &quot;Timeout&quot;: 3, \n    &quot;Runtime&quot;: &quot;nodejs10.x&quot;\n}\n\n\nLet&#39;s move on to creating a Rest Api.\n\n\nStep 7: Create Rest Api \n\n\nThis should be simple, let&#39;s update our CloudFormation template to have a Rest Api with /orders/{instanceId} as the resource and a GET method.\n\n\nAWSTemplateFormatVersion: &quot;2010-09-09&quot;\nResources:\n  ## Content Trimmed\n\n  OrdersApiGateway:\n    Type: AWS::ApiGateway::RestApi ## create a rest api\n    Properties:\n      Name: &quot;orders-api&quot;\n\n  OrdersResource:\n    Type: AWS::ApiGateway::Resource ## create an &quot;orders&quot; resource\n    Properties:\n      ParentId: !GetAtt &quot;OrdersApiGateway.RootResourceId&quot;\n      PathPart: &quot;orders&quot;\n      RestApiId: !Ref &quot;OrdersApiGateway&quot;\n\n  OrderIdResource:\n    Type: AWS::ApiGateway::Resource ## create an &quot;{orderId}&quot; resource\n    Properties:\n      ParentId: !Ref &quot;OrdersResource&quot;\n      PathPart: &quot;{orderId}&quot;\n      RestApiId: !Ref &quot;OrdersApiGateway&quot;\n\n  OrdersGetMethod:\n    Type: AWS::ApiGateway::Method ## create a &quot;GET&quot; method and integrate with lambda function\n    Properties:\n      HttpMethod: &quot;GET&quot;\n      AuthorizationType: &quot;NONE&quot;\n      RestApiId: !Ref &quot;OrdersApiGateway&quot;\n      ResourceId: !Ref &quot;OrderIdResource&quot;\n      Integration:\n        IntegrationHttpMethod: &quot;POST&quot;\n        Type: &quot;AWS_PROXY&quot;\n        Uri:\n          Fn::Join:\n            - &quot;&quot;\n            - - &quot;arn:&quot;\n              - &quot;aws&quot;\n              - &quot;:apigateway:&quot;\n              - Ref: AWS::Region\n              - :lambda:path/2015-03-31/functions/\n              - Fn::GetAtt:\n                  - OrdersFunction\n                  - Arn\n                  - /invocations\n\n\nExecuting npm t should now create Rest Api in LocalStack. We can verify the same -\n\n\n&amp;gt; aws apigateway get-rest-apis \\\n--query &quot;items[?name==&#39;orders-api&#39;].id&quot; \\\n--output text --region us-east-1 \\\n--endpoint-url=http://localhost:4567\n\n//Should print Api Id\n\n\nStep 8: Adding Integration Test for the application\n\n\nIn order to write this integration test we should be sending an http request to an endpoint exposed by Api Gateway inside LocalStack. This endpoint with LocalStack looks like -http://localhost:4567/restapis/&amp;lt;&amp;lt;Rest Api Id&amp;gt;&amp;gt;/test/_user_request_/orders/&amp;lt;&amp;lt;Order Id&amp;gt;&amp;gt;\n\n\nThis means we need a way to get the rest api id that was created as a part of deployment of CloudFormation template. We will add aws apigateway get-rest-apis command as a part of a script which will be executed from init.sh. This command will write the rest api id into a file which will be read by our integration test.\n\n\n#!/bin/sh\n\naws apigateway get-rest-apis \\\n--query &quot;items[?name==&#39;orders-api&#39;].id&quot; \\\n--output text --region us-east-1 \\\n--endpoint-url=http://localhost:4567 &amp;gt; rest_api_id\n\n\nThis is how our init.sh looks now -\n\n\n#!/bin/sh\n\n./createBucket.sh                       //creates a bucket\n./package.sh                            //copies the archive\n./deploy.sh                             //deploys the cloudformation\n./outputRestApiId.sh                    //logs the rest api id to a file\n\n\nNow, it is the time to add integration test.\n\n\nimport {OrderRepositoryFixture} from &quot;./fixture/OrderRepositoryFixture&quot;;\nimport {Order} from &quot;../src/model/Order&quot;;\n\nimport * as fs from &quot;fs&quot;;\nimport * as path from &quot;path&quot;;\nimport Axios from &quot;axios&quot;; //add axios as dev-dependency\n\nlet apiId = &quot;&quot;;\n\nbeforeEach(() =&amp;gt; {\n    //read rest api id\n    apiId = fs.readFileSync(path.resolve(&quot;test/infra/rest_api_id&quot;), &quot;utf8&quot;).trim();\n});\n\ntest(&quot;should return an order given there is AN order for the provided order id&quot;, async () =&amp;gt; {\n\n    const orderId = &quot;order-500&quot;;\n\n    await OrderRepositoryFixture.deleteAnOrder(orderId); //delete an existing order\n    await OrderRepositoryFixture.createAn(new Order(orderId, 4000)); //save an new order\n\n    //make an API call\n    const apiUrl = `http://localhost:4567/restapis/${apiId}/test/_user_request_/orders/${orderId}`;\n    const response = await Axios.get(apiUrl);\n\n    //assert on the response status and the content\n    expect(response.status).toEqual(200);\n    expect(response.data).toEqual({\n        &quot;orderId&quot;: orderId,\n        &quot;amount&quot;: 4000\n    });\n}, 20000);\n\n\nBefore the test can be run, we will have to make one change in DynamoDbConfiguration. It returns a dynamoDbClient which connects to dynamodb running on localhost:4569. This is not true anymore because lambda is running inside a docker container and for that lambda function &quot;localhost:4569&quot; will refer to the port 4569 on docker&#39;s IP. What we need is the port 4569 with the IP of host machine. So, let&#39;s make this change. This is how updated DynamoDbConfiguration will look like -\n\n\nimport {DynamoDB} from &quot;aws-sdk&quot;\n\nconst executionEnvironment = () =&amp;gt; {\n    const defaultExecutionEnvironment = &quot;local&quot;;\n    return process.env.ExecutionEnvironment || defaultExecutionEnvironment;\n};\n\nconst isExecutionEnvironmentLocal = () =&amp;gt; executionEnvironment() === &quot;local&quot;;\n\nexport const dynamoDbClient = () =&amp;gt; {\n    if (isExecutionEnvironmentLocal()) {\n        /** LOCALSTACK_HOSTNAME:\n        *     for accessing the hostname from inside the container\n        *   localhost: \n        *     for running repository integration tests which run on host machine\n        **/\n        const dynamoHost = process.env.LOCALSTACK_HOSTNAME || &quot;localhost&quot;;\n        return new DynamoDB({\n            &quot;region&quot;: &quot;us-east-1&quot;,\n            &quot;endpoint&quot;:`http://${dynamoHost}:4569`\n        });\n    } else {\n        return new DynamoDB({\n            &quot;region&quot;: &quot;ap-south-1&quot;\n        });\n    }\n};\n\n\nLocalStack exposes an environment variable LOCALSTACK_HOSTNAME which is available inside docker process that refers to the host machine.\n\n\nThat&#39;s it run all the tests npm t and see them pass 😁\n\n\nSummary\n\n\nWe used LocalStack to test our application. Everything is available here.\n\n\nHere is a quick glimpse of the sequence of events that happen when tests are executed.\n\n\n\n\n\nLet&#39;s move on to our last article and see everything in action on AWS account.\n\n"
} ,
  
  {
    "title"    : "Beginning Serverless Journey",
    "category" : "",
    "tags"     : " AWS Lambda, Serverless",
    "url"      : "/beginning-serverless-journey/",
    "date"     : "March 10, 2020",
    "excerpt"  : "\nServerless is a paradigm which lays its foundations on the fact that &quot;We don&#39;t have to provision and manage servers&quot;. \nThis article series explores various aspects involved in a serverless application lifecycle including - development, testing an...",
  "content"  : "\nServerless is a paradigm which lays its foundations on the fact that &quot;We don&#39;t have to provision and manage servers&quot;. \nThis article series explores various aspects involved in a serverless application lifecycle including - development, testing and deployment.\nOur serverless journey which starts from building to deploying an application will be using multiple serverless components including AWS Lambda, AWS API Gateway, AWS DynamoDB, LocalStack and AWS CDK.\n\n\n\nLet&#39;s deep dive step by step into what it takes to build a Serverless application.\n\n\nBuilding a Serverless application\n\n\nLet&#39;s assume a hypothetical &quot;Order Service&quot; that allows creation of an order and its retrieval by id.\n\n\nAs a part of this article we will be building just one part of this service which will expose a REST API to allow users to &quot;find an order&quot; by &quot;orderId&quot;.  Below diagram highlights different AWS components involved in finding an order by its id.\n\n\n\n\n\n\n\nWe will be using TypeScript for writing our Serverless application. Why Typescript? For a few reasons - \n\n\n\nSmall cold start time\nSupports static typing and type inference\nExisting Javascript libraries can be used with Typescript\n\n\n\nSo, let&#39;s start building our application. \n\n\nStep 1: Setting up the project\n\n\nLet&#39;s quickly setup our project -\n\n\n\nInstall typescript globally by executing npm install typescript -g\nCreate a directory named serverless-order-service representing our project\nExecute npm init -y inside our project directory (serverless-order-service)\nAdd typescript as a dependency by executing npm install typescript --save inside our project directory\n\n\n\nAs a final step, add tsconfig.json with a very simple configuration as mentioned below -\n\n\n{\n  &quot;compilerOptions&quot;: {\n    &quot;noEmitOnError&quot;: true,\n    &quot;moduleResolution&quot;: &quot;node&quot;,\n    &quot;module&quot;: &quot;commonjs&quot;,\n    &quot;target&quot;: &quot;es6&quot;,\n    &quot;outDir&quot;: &quot;dist&quot;,\n    &quot;inlineSourceMap&quot;: true\n  }\n}\n\n\nThat should be it. Our project set up is done and we are ready to make our first commit.\n\n\nBefore we start coding\n\n\nLet&#39;s take a moment to think about the overall design of the project. \n\n\nThis application is a classic web application which involves a REST API, a database and an object representing the persistent state of order. With this very small context, I feel it would not be unfair to organise the project in Model-View-Controller fashion which means execution of a user request will involve the following components - \n\n\n\n\n\n\n\nStep 2: Let&#39;s start with lambda handler\n\n\nLambda handler is a function which will be invoked by AWS Lambda Service in response to an event. An event could be - an object uploaded on an S3 bucket, an event on SQS or an https request via API gateway and many more. In our example a request to an API Gateway will be an event.\n\n\nBefore we start our lambda function let&#39;s install type definition for aws-lambda by executing - npm install @types/aws-lambda --save-dev and create a commit.\n\n\nAfter the dependency is installed we are ready to code our handler. Let&#39;s put this in a file named handler.ts under src directory.\n\n\nimport {APIGatewayEvent} from &quot;aws-lambda&quot;;\nexport const ordersHandler = async (event: APIGatewayEvent): Promise&amp;lt;any&amp;gt; =&amp;gt; {\n    //your code goes here\n};\n\n\nThis is the simplest handler function that could be created at this stage. One thing to note is we are using the type APIGatewayEvent imported from &quot;aws-lambda&quot; to get type-safe events as parameter to handler function.\n\n\nWe want to keep our handler function as thin as possible so we will delegate the request to a controller class which instead of taking APIGatewayEvent will take a domain object that wraps APIGatewayEvent.\n\n\nIn this example, OrderRequest is that domain object, effectively a wrapper over APIGatewayEvent.\n\n\nimport {APIGatewayEvent} from &quot;aws-lambda&quot;;\n\nexport const ordersHandler = async (event: APIGatewayEvent): Promise&amp;lt;any&amp;gt; =&amp;gt; {\n    return new OrderController().handle(new OrderRequest(event)); //handler invokes controller\n};\n\nexport class OrderController {\n    handle(orderRequest: OrderRequest) { //accepts OrderRequest\n    }\n}\n\n//domain object which wraps APIGatewayEvent\nexport class OrderRequest {\n    constructor(private readonly event: APIGatewayEvent) {\n    }\n}\n\n\nLet&#39;s move OrderController and OrderRequest classes to controller and model packages (or directories) respectively and invoke OrderController from handler. This is how the handler function will look like after the classes have been moved.\n\n\nimport {APIGatewayEvent} from &quot;aws-lambda&quot;;\nimport {OrderRequest}    from &quot;./model/OrderRequest&quot;;\nimport {OrderController} from &quot;./controller/OrderController&quot;;\n\nexport const ordersHandler = async (event: APIGatewayEvent): Promise&amp;lt;any&amp;gt; =&amp;gt; {\n    return new OrderController().handle(new OrderRequest(event));\n};\n\n\nI think we are ready to make our next commit. I know we haven&#39;t written any unit tests but as far as this article is concerned, we will write unit tests before we start the next one.\n\n\nStep 3: Let&#39;s jump into controller\n\n\nController is a class which will handle the incoming request and return an appropriate response. By this definition, controller will be expected to accept an OrderRequest and find an order by its id.\n\n\nimport {OrderRequest} from &quot;../model/OrderRequest&quot;;\n\nexport class OrderController {\n\n    handle(orderRequest: OrderRequest) {\n       if (orderRequest.isAGetOrder()) { //if the request is for finding an order\n         return this.findAnOrderBy(orderRequest.orderId()); //find an order by its id\n       }\n       return null;\n    }\n\n    private findAnOrderBy(id: string): Order {\n        return null; //fake implementation\n    }\n}\n\nexport class Order {\n}\n\n\nFew quick observations - \n\n\n\nOrderRequest is the domain object which encapsulates APIGatewayEvent and provides domain methods like orderId(), isAGetOrder() without exposing APIGatewayEvent\nCurrently orderId() and isAGetOrder() methods of OrderRequest return fixed (or fake) values\n\n\n\nLet&#39;s make a few quick changes in OrderController -\n\n\n\nMove Order class into model package\nInvoke Service method to find an order by its id\n\n\n\nThis is how different classes look at this stage.\n\n\n//OrderController.ts\nimport {OrderRequest} from &quot;../model/OrderRequest&quot;;\nimport {Order}        from &quot;../model/Order&quot;;\nimport {OrderService} from &quot;../service/OrderService&quot;;\n\nexport class OrderController {\n    private orderService: OrderService;\n\n    constructor() {\n        this.orderService = new OrderService();\n    }\n\n    handle(orderRequest: OrderRequest) {\n       if (orderRequest.isAGetOrder()) {\n         return this.findAnOrderBy(orderRequest.orderId())\n       }\n       return null;\n    }\n\n    private findAnOrderBy(id: string): Order {\n        return this.orderService.findAnOrderBy(id); //controller invokes service to find an order by its id\n    }\n}\n\n//OrderRequest.ts\nimport {APIGatewayEvent} from &quot;aws-lambda&quot;;\n\nexport class OrderRequest {\n    constructor(private readonly event: APIGatewayEvent) {\n    }\n\n    isAGetOrder(): boolean {\n       return true; //fake implementation\n    }\n    orderId(): string {\n        return &quot;&quot;; //fake implementation\n    }\n}\n\n//OrderService.ts\nexport class OrderService {\n\n    findAnOrderBy(id: string) {\n        return null; //fake implementation\n    }\n}\n\n\nStep 4: Let&#39;s jump into service\n\n\nService layer will interact with repository to find an order by id. It doesn&#39;t look like service layer is really needed for this example, but let&#39;s proceed with it -\n\n\nexport class OrderService {\n    private repository: OrderRepository;\n\n    constructor() {\n        this.repository = new OrderRepository();\n    }\n\n    findAnOrderBy(id: string) {\n        return this.repository.findAnOrderBy(id); //service invokes repository to find an order by its id\n    }\n}\n\nexport class OrderRepository {\n    findAnOrderBy(id: string) {\n        return null; //fake implementation\n    }\n}\n\n\nLet&#39;s move OrderRepository into repository package.\n\n\nStep 5: Let&#39;s jump into repository\n\n\nRepository will interact with our database which in this example is a DynamoDB and fetch an order by its id. Let&#39;s assume a table named &quot;orders&quot; with &quot;orderId&quot; as the HASH key and an attribute named &quot;amount&quot;.\n\n\nWe will be using &quot;aws-sdk&quot; for querying DynamoDB. &quot;aws-sdk&quot; is a dependency which is available in the runtime environment of lambda which means this dependency can be added as a &quot;devDependency&quot;.\n\n\nSo, let&#39;s add it as a &quot;devDependency&quot; by executing npm install aws-sdk --save-dev. Let&#39;s also add type definitions for aws-sdk by executing npm install @types/aws-sdk --save-dev.\n\n\nNow we are ready to query &quot;orders&quot; table.\n\n\nimport {DynamoDB} from &quot;aws-sdk&quot;\nimport {GetItemInput} from &quot;aws-sdk/clients/dynamodb&quot;;\nimport {Order} from &quot;../model/Order&quot;;\n\nconst dynamoDb = new DynamoDB({\n    &quot;region&quot;: &quot;ap-south-1&quot;\n});\n\nexport class OrderRepository {\n\n    async findAnOrderBy(id: string) {\n        const getItemInputOptions: GetItemInput = {\n            TableName: &quot;orders&quot;, //table name\n            Key: {\n                &quot;orderId&quot;: {S: id} //query against orderId attribute of order item\n            }\n        };\n        const response = await dynamoDb.getItem(getItemInputOptions).promise(); //get a dynamo item by passing its id\n        return response.Item;\n    }\n}\n\n\nFew quick observations - \n\n\n\nWe have hard-coded the region and table name, which we might want to fetch from configuration / properties for below mentioned reasons -\n\nif region is different for actual deployment and integration testing (using localstack)\n if there are multiple deployment environments and dynamo table name is different for each environment\n\nWith DynamoDB lowest level of abstraction is a table, hence, we might need different table name for each environment\n\n\n\n\nMethod name and return type of the method do not go hand-in-hand. We expect this method to return an &quot;order&quot; but this method seems to be returning some type specified by &quot;response.Item&quot;\n\n\n\nLet&#39;s quickly make a change to return Order instead of response.Item. \n\n\nimport {DynamoDB} from &quot;aws-sdk&quot;\nimport {GetItemInput} from &quot;aws-sdk/clients/dynamodb&quot;;\nimport {Order} from &quot;../model/Order&quot;;\n\nconst dynamoDb = new DynamoDB({\n    &quot;region&quot;: &quot;ap-south-1&quot;\n});\n\nexport class OrderRepository {\n\n    async findAnOrderBy(id: string) {\n        const getItemInputOptions: GetItemInput = {\n            TableName: &quot;orders&quot;, //table name\n            Key: {\n                &quot;orderId&quot;: {S: id} //query against orderId attribute of order item\n            }\n        };\n        const response = await dynamoDb.getItem(getItemInputOptions).promise(); //get a dynamo item by passing its id\n        return response.Item ? Order.from(response.Item) : null;  //map dynamo item to Order\n    }\n}\n\n\nand this is how Order.ts looks like - \n\n\nimport {DocumentClient} from &quot;aws-sdk/clients/dynamodb&quot;;\n\nexport class Order {\n    static from(item: DocumentClient.AttributeMap): Order {\n        return null; //fake implementation\n    }\n}\n\n\nThis completes our repository. We still have some open items. Let&#39;s take them one by one - \n\n\n\nPending implementation of from() in Order\nPending implementation of orderId() in OrderRequest\nPending changes relating to async/await in controller, service and handler\n\n\n\nStep 6: Finishing Order class\n\n\nOrder class provides a static method which accepts and instance of DocumentClient.AttributeMap and returns an instance of Order consisting of orderId and amount.\n\n\nimport {DocumentClient} from &quot;aws-sdk/clients/dynamodb&quot;;\n\nexport class Order {  \n\n    static from(item: DocumentClient.AttributeMap): Order {\n        return new Order(item.orderId.S, Number(item.amount.N)); //create an instance of Order from an instance of AttributeMap\n    }\n    private constructor(private readonly orderId: string,\n                        private readonly amount: number) {\n    }\n}\n\n\nAttributeMap within aws-sdk is defined as -\n\n\nexport type AttributeMap = {[key: string]: AttributeValue};\n\n\nand AttributeValue is an interface which is defined as -\n\n\nexport interface AttributeValue {\n    S?: StringAttributeValue;\n    N?: NumberAttributeValue;\n  .....\n}\n\n\nHence, item.orderId gives us an instance of AttributeValue and then we use .S or .N to get the corresponding value\n\n\nStep 7: Finishing OrderRequest class\n\n\n\nisAGetOrder() should return TRUE given a GET request beginning with /orders as the path part\norderId() should return the value of pathParameter &quot;orderId&quot;\n\n\n\nimport {APIGatewayEvent} from &quot;aws-lambda&quot;;\n\nexport class OrderRequest {\n    constructor(private readonly event: APIGatewayEvent) {\n    }\n\n    //return true if the request is a GET request, with path starting from /orders and containing a path parameter\n    isAGetOrder(): boolean {\n        return this.event.httpMethod === &quot;GET&quot; &amp;amp;&amp;amp;\n               this.event.path.startsWith(&quot;/orders&quot;) &amp;amp;&amp;amp;\n               this.event.pathParameters.orderId != null;\n    }\n    orderId(): string {\n        return this.event.pathParameters.orderId; //order id is passed as path parameter\n    }\n}\n\n\nStep 8: Introducing async/await in service and controller\n\n\nLet&#39;s introduce async and await in handler, controller and service.\n\n\n//handler.ts\nexport const ordersHandler = async (event: APIGatewayEvent): Promise&amp;lt;any&amp;gt; =&amp;gt; {\n    return await new OrderController().handle(new OrderRequest(event));\n};\n\n//OrderController.ts\nasync handle(orderRequest: OrderRequest) {\n    if (orderRequest.isAGetOrder()) {\n        return await this.findAnOrderBy(orderRequest.orderId())\n    }\n    return null;\n}\n\nprivate async findAnOrderBy(id: string) {\n    return await this.orderService.findAnOrderBy(id);\n}\n\n//OrderService.ts\nasync findAnOrderBy(id: string) {\n    return await this.repository.findAnOrderBy(id);\n}\n\n\nStep 9: Lambda response with API gateway\nWhen AWS lambda works behind an API gateway, it is expected to return a response in a specific format. This looks like -\n{\n   &quot;statusCode&quot;: Http Status Code,\n   &quot;body&quot;: Response body,\n   &quot;headers&quot;: Response headers\n}\n\n\nIt would be great if controller knows the least about this structure. All it should do is return a response with Order object. Let&#39;s create an abstraction which takes an object T and knows about the final HTTP response. Let&#39;s name this abstraction as Response.\n\n\nexport class Response&amp;lt;T&amp;gt; {\n    private constructor(readonly status: HttpStatus, readonly body?: T) {\n    }\n\n    //signifies 200 response\n    static ok&amp;lt;T&amp;gt;(body: T) {\n        return new Response(HttpStatus.OK, body);\n    }\n    //signifies 404 response\n    static notFound() {\n        return new Response(HttpStatus.NOT_FOUND);\n    }\n}\n\nexport class HttpStatus {\n    static readonly OK = &quot;200&quot;;\n    static readonly NOT_FOUND = &quot;404&quot;;\n}\n\n\nWith the introduction of Response, following will be the view of some classes - \n\n\nimport {OrderRequest} from &quot;../model/OrderRequest&quot;;\nimport {OrderService} from &quot;../service/OrderService&quot;;\n\nexport class OrderController {\n    private orderService: OrderService;\n\n    constructor() {\n        this.orderService = new OrderService();\n    }\n\n    async handle(orderRequest: OrderRequest): Promise&amp;lt;Response&amp;lt;Order | unknown&amp;gt;&amp;gt; {\n        if (orderRequest.isAGetOrder()) {\n            const order = await this.findAnOrderBy(orderRequest.orderId());\n            //return an Ok response if order is found else NotFound\n            return order === null ? Response.notFound() : Response.ok&amp;lt;Order&amp;gt;(order);\n        }\n        return Response.notFound(); //return NotFound response\n    }\n\n    private async findAnOrderBy(id: string) {\n        return await this.orderService.findAnOrderBy(id);\n    }\n}\n\nexport class Response&amp;lt;T&amp;gt; {\n    private constructor(readonly status: HttpStatus, readonly body?: T) {\n    }\n\n    static ok&amp;lt;T&amp;gt;(body: T) {\n        return new Response(HttpStatus.OK, body);\n    }\n    static notFound() {\n        return new Response(HttpStatus.NOT_FOUND);\n    }\n    get() {\n        //return a well formed JSON response\n        return this.body === null ? {&quot;statusCode&quot;: this.status} : {\n            &quot;statusCode&quot;: this.status,\n            &quot;body&quot;: JSON.stringify(this.body)\n        }\n    }\n}\n\nexport class HttpStatus {\n    static readonly OK = &quot;200&quot;;\n    static readonly NOT_FOUND = &quot;404&quot;;\n}\n\n\nIf controller returns an instance of Response, handler code can be changed to invoke get() on the returned instance.\n\n\nimport {APIGatewayEvent} from &quot;aws-lambda&quot;;\nimport {OrderRequest}    from &quot;./model/OrderRequest&quot;;\nimport {OrderController} from &quot;./controller/OrderController&quot;;\n\nexport const ordersHandler = async (event: APIGatewayEvent): Promise&amp;lt;any&amp;gt; =&amp;gt; {\n    const response = await new OrderController().handle(new OrderRequest(event));\n    return response.get(); //handler invokes get() on the response returned from controller\n};\n\n\nThat&#39;s it. We have connected all the pieces, yes without any form of tests.\n\n\nStep 10: Adding unit tests\n\n\nLet&#39;s add a couple of unit tests before we close the article. I will add all the necessary unit tests offline and commit the code. Before we can start with tests let&#39;s add following dependencies -\n\n\n\nnpm install jest --save-dev\nnpm install @types/jest --save-dev\nnpm install ts-jest --save-dev\nnpm install sinon --save-dev\n\n\n\njest configuration in jest.config.js -\n\n\nmodule.exports = {\n    &quot;testMatch&quot;: [\n        &quot;**/__tests__/**/*.+(ts|tsx|js)&quot;,\n        &quot;**/?(*.)+(spec|test).+(ts|tsx|js)&quot;\n    ],\n    &quot;transform&quot;: {\n        &quot;^.+\\\\.(ts|tsx)$&quot;: &quot;ts-jest&quot;\n    },\n};\n\n\ntest script in package.json -\n\n\n&quot;scripts&quot;: {\n  &quot;test&quot;: &quot;jest test/**&quot;\n}\n\n\nController unit tests\n\n\nLet&#39;s add our first test which attempts to validate the status for finding an order by its id.\n\n\nimport {OrderController} from &quot;../../src/controller/OrderController&quot;;\nimport {OrderRequest} from &quot;../../src/model/OrderRequest&quot;;\nimport {APIGatewayEvent} from &quot;aws-lambda&quot;;\nimport {HttpStatus} from &quot;../../src/model/ModelAndResponseStatus&quot;;\nimport {OrderService} from &quot;../../src/service/OrderService&quot;;\nimport {Order} from &quot;../../src/model/Order&quot;;\n\nimport * as sinon from &quot;sinon&quot;;\n\ntest(&quot;should return Ok as the response status given a request to find an order by id&quot;, async () =&amp;gt;{\n    sinon.stub(OrderService.prototype, &quot;findAnOrderBy&quot;)\n         .callsFake(() =&amp;gt; sinon.mock(Order));\n\n    const response = await new OrderController().handle(orderRequest(&quot;id-100&quot;));\n\n    expect(response.status).toEqual(HttpStatus.OK);\n});\n\nafterEach(() =&amp;gt; {\n    sinon.restore();\n});\n\nconst orderRequest = (id: string) =&amp;gt; {\n    const apiGatewayEvent: APIGatewayEvent = {\n        httpMethod: &quot;GET&quot;,\n        path: `/orders/${id}`,\n        pathParameters: {\n            &quot;orderId&quot;: id\n        },\n        body: null,\n        isBase64Encoded: false,\n        headers: {},\n        multiValueHeaders: {},\n        queryStringParameters: {},\n        multiValueQueryStringParameters: {},\n        stageVariables: {},\n        requestContext: null,\n        resource: &quot;&quot;\n    };\n    return new OrderRequest(apiGatewayEvent);\n};\n\n\nOne quick observation -\n\n\n\napiGatewayEvent had to be constructed with all the attributes even though we needed only pathParameters because APIGatewayEvent type mandates all the attributes\nvisibility of status and model (in the next test) had to be changed from private to public to assert on these fields\n\n\n\nAnother test could be to check the order returned from controller given an id.\n\n\ntest(&quot;should return an order given a request to find an order by id&quot;, async () =&amp;gt; {\n    sinon.stub(OrderService.prototype, &quot;findAnOrderBy&quot;)\n         .callsFake(() =&amp;gt; new Order(&quot;id-100&quot;, 1445));\n\n    const response = await new OrderController().handle(orderRequest(&quot;id-100&quot;));\n\n    expect(response.body!!).toEqual(new Order(&quot;id-100&quot;, 1445));\n});\n\n\nI guess we are ready to do TDD as well for Serverless.\n\n\nSummary\n\n\nFinally we have come to an end of our first article where we made an attempt to design a small part of a serverless application which uses AWS Lambda, API Gateway and DynamoDB.\n\n\nAs a part of this application we have tried to draw some parallels with MVC design pattern and bring the same to the serverless world.\n\n\nItems that we have left -\n\n\n\nException handling is missing\nController checks if the request is for getting an order. This if/else ladder will grow given the same lambda handles creation and deletion of orders.\nEvery component is unit testable in itself, except Repository layer which needs dynamo db\n\nI am sure you will be able to fill these gaps and at this stage, I will move forward.\n\nThere is a lot of work still left before we can deploy the code -\n\n\n\nWe need to have integration test(s) which can give us confidence if this entire application is actually working or not\nWe need to integrate CDK (Cloud Development Kit) for deploying our infrastructure\nWe need to have unit tests and snapshot tests for our CDK based infra code\n\n\n\nCode is available here. \n\n\nLet&#39;s move on to our next article which explores integration testing using Localstack for our serverless application.\n"
} ,
  
  {
    "title"    : "Invest In Blogging",
    "category" : "",
    "tags"     : " Blogging",
    "url"      : "/invest-in-blogging/",
    "date"     : "June 7, 2019",
    "excerpt"  : "\nWe often have a lot to share with people, this could be our learnings, our opinions and our experiences. There are times when we feel the need to get our ideas validated or get feedback from people. These are definitely some of the reasons to inv...",
  "content"  : "\nWe often have a lot to share with people, this could be our learnings, our opinions and our experiences. There are times when we feel the need to get our ideas validated or get feedback from people. These are definitely some of the reasons to invest in blogging and connect with community.\n\n\n\nLet&#39;s see some reasons for investing in writing blogs.\n\n\n\n\n\n\n\nHelp people learn from your learning journey\n\n\nThere are times when we often feel &quot;it would have been great if someone had written an article to explain a concept&quot;, start writing if you have had this feeling. \n\n\nLearning is like climbing a rock. While climbing, we always look at the tip of the rock just to realize it is too far away. What is also important is to realize that there are people who might have just started this journey and your &quot;learning journey&quot; could go a long way in helping them.\n\n\nHelp people learn from your mistakes\n\n\nShare your mistakes with the community. We as a community learn from each other&#39;s mistakes and experiences and these things are really valuable. \n\n\nYour blog on &quot;Failing with Microservices&quot; could help me in avoiding some mistakes or at least rethink my design if I am starting with microservices.\n\n\nGet feedback from community\n\n\nThere are a lot of things which help us grow as an individual and one of them is feedback or I should say &quot;Constructive Feedback&quot;. Write to get feedback from community, to get their thoughts, to hear their experiences and to learn from all of these. Let&#39;s see how this could work. \n\n\nSay, I am very excited to use Coroutines to build reactive streams in my next project and I share an article &quot;Being Reactive with Kotlin Coroutines&quot; which talks about the basics of Coroutines and abstractions like &quot;Channel&quot; to implement reactive streams. \n\n\nThis article receives a lot of feedback and one of the feedback says - \n\n\nHey, nicely put. I would suggest you to check this link. It says -\n&quot;There is no way to receive the elements from a channel again. The channel is closed when the producer coroutine is over and the attempt to receive from it again cannot receive anything.&quot;\nYou might also want to take a look at kotlinx-coroutines-rx2.\n\n\n\nNow, this is important as it helps me understand a lot of dimensions including backpressure, hot and cold observables which I had not considered. Thanks to the feedback, I got pointed in the right direction.\n\n\n. . . \n\n\nInvestment is tricky and one expects a return from every investment. Let&#39;s see the overall &quot;return over investment&quot; in blogging. \n\n\nSolidifies your understanding\n\n\nLike we learn when we teach people, we also learn when we share our ideas with people. Blogging helps in solidifying our understanding and the reason I say this is - \n\n\nWe try to communicate our ideas in the simplest possible manner to our readers. In order to do this, we choose to take small steps and each of these steps is well thought of and analyzed. Each step in turn teaches us something which improves our understanding.\n\n\n\nWe were talking about DSLs in Kotlin in one of the workshops and I happened to like the way that topic was built - from lambdas to extension function to lambdas with receiver to invoke function. \n\n\nI decided to share the same in an article Kotlin DSLs: The Basics and if I look back, I realize these two things - a workshop and an article have really helped me understand Kotlin DSLs well.\n\n\nIncreases your confidence\n\n\nYou are not afraid of reaching out to people and you are not the same person you used to be who would think &quot;should I share this, people would already know it&quot;, &quot;this tech was released 5 years back and I am writing about it now, does it make sense?&quot;. You become someone who would share his/her ideas with confidence.\n\n\nChallenges you to write better every time\n\n\nYou challenge yourself to write better every time. You tend to experiment with different styles of writing in an attempt to communicate your ideas clearly and connect with people better.\n\n\n Makes you a better articulator of thoughts\n\n\nYou tend to wear a writer&#39;s hat every time you sit to share something. An attempt is made is to talk to the readers through your article which acts like a story. You read your article hundreds of times in an attempt to articulate better. All this does is make you a better articulator of thoughts.\n\n\nBuilds your network\n\n\nInvestment in blogging is a great way to build network, you get to know people and people get to know you. \n\n\nNetworking is very powerful and truly magical, it can surprise you with a lot of opportunities which you might not imagine. You might get to speak at conferences, work with people that you follow and many more. \n\n\nInvestment in blogging is really a simple way to build network !!\n\n\nBuilds your brand\n\n\nInvestment in blogging acts as a great tool to build your and your organization&#39;s brand.\n\n\n&quot;Return over investment in blogging&quot; looks promising but we need to be aware of the fact that a return might not be immediate for an investment. \nMaking an investment is the first step and usually the most difficult step, rest is all about return ;-)\nTake your first step with blogging, share your ideas / opinions / thoughts with the community. It is a great tool which does a lot of magic, has got great return and more importantly &quot;it is fun&quot;. \nInvest in blogging.\n\n\n\nSummary\n\n\n\n\n"
} ,
  
  {
    "title"    : "Kotlin DSL",
    "category" : "",
    "tags"     : " Domain Specific Language, DSL, Kotlin",
    "url"      : "/kotlin-dsl/",
    "date"     : "May 27, 2018",
    "excerpt"  : "A domain-specific language (DSL) is a computer language specialized to a particular application domain. This is in contrast to a general-purpose language (GPL), which is broadly applicable across domains. There are a wide variety of DSLs, ranging ...",
  "content"  : "A domain-specific language (DSL) is a computer language specialized to a particular application domain. This is in contrast to a general-purpose language (GPL), which is broadly applicable across domains. There are a wide variety of DSLs, ranging from widely used languages for common domains, such as HTML for web pages, down to languages used by only one or a few pieces of software.\nKotlin DSL\nKotlin provides first class support for DSL which allows us to express domain-specific operations much more concisely than an equivalent piece of code in a general-purpose language.\nLet&#39;s try and build a simple DSL in Kotlin -\ndependencies {\n   compile(&quot;io.arrow-kt:arrow-data:0.7.1&quot;)\n   compile(&quot;io.arrow-kt:arrow-instances-core:0.7.1&quot;)\n   testCompile(&quot;io.kotlintest:kotlintest-runner-junit5:3.1.0&quot;)\n}\nThis should be familiar to people using gradle as their build tool. Above DSL specifies compile and testCompile dependencies for a gradle project in very concise and expressive form.\nHow does Kotlin support DSL\nBefore we get in to Kotlin&#39;s support for DSL, let&#39;s look at lambdas in Kotlin.\nfun buildString(action: (StringBuilder) -&amp;gt; Unit): String {\n   val sb = StringBuilder()\n   action(sb)\n   return sb.toString()\n}\nbuildString() takes a lambda as a parameter (called action) and invokes it by passing an instance of StringBuilder. Any client code which invokes buildString() will look like the below code -\nval str = buildString {\n    it.append(&quot;Hello&quot;)\n    it.append(&quot; &quot;)\n    it.append(&quot;World&quot;)\n}\nA few things to note here -\n\nbuildString() takes lambda as the last parameter. If a function takes lambda as the last parameter, Kotlin allows you to invoke the function using braces { .. }, no need of using parentheses\nit is the implicit parameter available in lambda body which is an instance of StringBuilder in this example\n\nThis information is good enough to write a gradle dependencies DSL.\nFirst Attempt at DSL\nIn order to build a gradle dependencies DSL we need a function called dependencies which should take a lambda of type T as a parameter where T provides compile and testCompile functions.\nLet&#39;s try -\nfun dependencies(action: (DependencyHandler) -&amp;gt; Unit): DependencyHandler {\n    val dependencies = DependencyHandler()\n    action(dependencies)\n    return dependencies\n}\n\nclass DependencyHandler {\n    fun compile(coordinate: String){\n        //add coordinate to some collection\n    }\n    fun testCompile(coordinate: String){\n        //add coordinate to some collection\n    }\n}\ndependencies is a simple function which takes a lambda accepting an instance of DependencyHandler as an parameter and returning Unit. DependencyHandler is the type T which has compile and testCompile functions.\nClient code for the above concept will look like -\ndependencies {\n    it.compile(&quot;&quot;) //it is an instance of DependencyHandler\n    it.testCompile(&quot;&quot;)\n}\nAre we done? Not really. The problem is the implicit parameter it used in the client code. Can we remove it? In order to remove implicit parameter, we need to look at &quot;Lambda With Receiver&quot;.\nLambda With Receiver\nReceiver is a simple type in Kotlin which is extended.\nLet&#39;s see this with an example -\nfun String.lastChar() : Char =\n                   this.toCharArray().get(this.length - 1)\nWe have extended String to have lastChar() as a function which means we can always invoke it as -\n&quot;Kotlin&quot;.lastChar()\nHere, String is the receiver type and this used in the body of lastChar() is the receiver object. Can we combine these 2 concepts - lambda and receiver?\nLet&#39;s rewrite our buildString function using lambda with receiver -\nfun buildString(action: StringBuilder.() -&amp;gt; Unit): String {\n    val sb = StringBuilder()\n    sb.action()\n    return sb.toString()\n}\nA few things to note here -\n\nbuildString() takes a lambda with receiver as an parameter\nStringBuilder is the receiver type in the lambda\nthe way we invoke action function is different this time. Because action is an extension function on StringBuilder we invoke it using sb.action(), where sb is an instance of StringBuilder\n\nLet&#39;s create a client of buildString function -\nval str = buildString {\n    this.append(&quot;Hello&quot;) //this here is an instance of StringBuilder\n    append(&quot; &quot;)\n    append(&quot;World&quot;)\n}\nIsn&#39;t this brilliant? Client code will always have access to this while invoking a function which takes lambda with receiver as a parameter.\nShall we rewrite our gradle dependencies DSL code?\nAnother Attempt at DSL\nfun dependencies(action: DependencyHandler.() -&amp;gt; Unit): DependencyHandler {\n    val dependencies = DependencyHandler()\n    dependencies.action()\n    return dependencies\n}\n\nclass DependencyHandler {\n    fun compile(coordinate: String){\n        //add coordinate to some collection\n    }\n    fun testCompile(coordinate: String){\n        //add coordinate to some collection\n    }\n}\nThe only change we have done here is in dependencies function which takes a lambda with receiver as the parameter. DependencyHandler is the receiver type in action parameter which means the client code will always have access to the instance of DependencyHandler.\nLet&#39;s see the client code -\ndependencies {\n    compile(&quot;&quot;) //same as this.compile(&quot;&quot;)\n    testCompile(&quot;&quot;)\n}\nWe are  able to create a DSL using Lambda with Receiver as a parameter to a function.\nOperator Function invoke()\nKotlin provides an interesting function called invoke which is an operator function. Specifying invoke operator on a class allows it to be called on any instances of the class without a method name.\nLet&#39;s see this in action -\nclass Greeter(val greeting: String) {\n    operator fun invoke(name: String) {\n        println(&quot;$greeting $name&quot;)\n    }\n}\n\nfun main(args: Array&amp;amp;lt;String&amp;amp;gt;) {\n    val greeter = Greeter(greeting = &quot;Welcome&quot;)\n    greeter(name = &quot;Kotlin&quot;)  //this calls the invoke function which takes String as a parameter\n}\nA few things to note about invoke() here -\n\nis an operator function\ntakes parameter\ncan be overloaded\nis being called on the instance of Greeter class without method name\n\nLet&#39;s use invoke in building DSL -\nBuilding DSL using invoke function\nclass DependencyHandler {\n    fun compile(coordinate: String){\n        //add coordinate to some collection\n    }\n    fun testCompile(coordinate: String){\n        //add coordinate to some collection\n    }\n    operator fun invoke(action: DependencyHandler.() -&amp;gt; Unit): DependencyHandler {\n        this.action()\n        return this\n    }\n}\nWe have defined an operator function in DependencyHandler which takes a lambda with receiver as a parameter. This means invoke will automatically be called on instance(s) of DependencyHandler and client code will have access to instance of DependencyHandler.\nLet&#39;s write the client code -\nval dependencies = DependencyHandler()\ndependencies { //as good as dependencies.invoke(..)\n   compile(&quot;&quot;)\n   testCompile(&quot;&quot;)\n}\ninvoke() can come in handy while building DSL.\nConclusion\n\nKotlin provides a first class support for DSL which is type safe\nOne can create a DSL in Kotlin using -\n\nLambda as function parameters\nLambda With Receiver as function parameter\noperator function invoke along with Lambda With Receiver as function parameter\n\n\n\nReferences\n\nKotlin In Action\n"
} ,
  
  {
    "title"    : "Kotlin Wishlist for Java",
    "category" : "",
    "tags"     : " Kotlin",
    "url"      : "/kotlin-wishlist-for-java/",
    "date"     : "April 20, 2018",
    "excerpt"  : "There is no doubt that Java has enjoyed a superior position when it comes to programming languages and is considered as one of the most important languages for development. However, there have been a number of languages developed on top of the JVM...",
  "content"  : "There is no doubt that Java has enjoyed a superior position when it comes to programming languages and is considered as one of the most important languages for development. However, there have been a number of languages developed on top of the JVM, like Kotlin.\nKotlin is a statically typed programming language for modern multi-platform applications. While I have been a Java developer for quite a long while, working on a project data-anonymization made me feel that there are things that Java should consider importing from Kotlin.\nThese are some of the Kotlin features that I would love to see making a place in Java.\nPromote Immutability\nJava 9 promotes immutability by introducing factory methods to create collections. It would be great to see immutable collections embedded in the language, rather than relying on wrappers to generate immutable collections. existingDepartments() is a function that returns an immutable list of Strings in Kotlin.\n//Kotlin\nfun existingDepartments(): List =\n    listOf(&quot;Human Assets&quot;, &quot;Learning &amp; Development&quot;, &quot;Research&quot;)\nJava 9 comes closest to returning an immutable list by throwing an UnsupportedOperationException when an attempt is made to add or remove an element from the list. It would be great to have a separate hierarchy of mutable and immutable collections and avoid exposing add/remove or any other mutating methods from immutable collections.\n//pre Java 8\npublic List existingDepartments() {\n    return new ArrayList();\n}\n//Java 8\npublic List existingDepartments() {\n    return Stream.of(&quot;Human Assets&quot;, &quot;Learning &amp; Development&quot;, &quot;Research&quot;)\n                 .collect(Collectors.toList());\n}\n//Java 9\npublic List existingDepartments() {\n    return List.of(&quot;Human Assets&quot;, \n                   &quot;Learning &amp; Development&quot;,\n                   &quot;Research&quot;);\n}\nBeing more explicit about immutable collections and letting immutable collections speak out loud for themselves should be given preference over exposing methods and throwing UnsupportedOperationExceptions\nMethod Parameters Should Be Final by Default\nWith an intent to promote immutability and avoid errors because of mutation, it might be worth to at least giving a thought to making method parameters final by default.\n//Kotlin\nfun add (augend: Int, addend: Int) = augend + addend\nParameters for the add() function are val by default cannot be changed, which means as a client of any function, I can rest assured that the function is not changing the arguments (not to be confused with object mutation) that are passed to it.\nMaking method parameters final by default might and will most likely break existing code bases on Java upgrades but is worth giving a thought\nHandle NULL at Compile Time\nEvery Java developer is bound to know the infamous NullPointerException. Kotlin took a major step by handling NULLs at compile time. Everything is non-null be default until it is explicitly stated.\nDid Java 8 not introduce Optional for the very same reason ? Let&#39;s see with an example:\n//Kotlin\nclass Employee(private val id: Int, private val department: Department?) {\n    fun departmentName() = department?.name ?: &quot;Unassigned&quot;\n}\nclass Department(val name: String)\n\n/**\n    Employee needs a non-nullable &quot;id&quot; and an optional department to be constructed.\n    val employee = Employee(null, null); &amp;lt;b&amp;gt; Compile Time Error &amp;lt;/b&amp;gt;\n**/\nThe Employee class has a primary constructor with a non-nullable id and an optional (nullable) department. Passing null for the id will result in a compile time error.\nThe departmentName() function accesses the name property of Department using the optional operator ? on the nullable field. If department is null, name will not be accessed and the expression on the left-hand side [department?.name] will return null. The Elvis operator ?: will return the right hand side (&quot;Unassigned&quot;) if the left-hand side of the expression is null.\n//Java 8\nclass Employee {\n    private Integer id;\n    private Optional department\n\n    Employee(Integer id, Optional department){\n       this.id = id;\n       this.department = department;\n    }\n    public String departmentName() {\n       return department.orElse(&quot;Unassigned&quot;);\n    }\n}\n\n/**\n    Employee needs a non-nullable &quot;id&quot; and an optional department to be constructed.\n    Employee employee = new Employee(null, null); &amp;lt;b&amp;gt;NPE !!!&amp;lt;/b&amp;gt;\n**/\nOptional will not protect the code from NPE, but Optional has its advantages:\n\nIt makes the domain model clear. The Employee class has an optional department, which is good enough to conclude that every employee may not be assigned a department\nIt promotes composability as in the departmentName() method\n\nHandling NULLs at compile time should result in cleaner code by removing unnecessary NULL checks in the form of an if statement, Objects.requireNonNull, Preconditions.checkNotNull, any other form\nTo keep things simple, department was passed in to the constructor even though this is an optional attribute.\nImprove Lambdas\nJava 8 introduced lambdas, which are built on top of a functional interface and a functional descriptor, meaning every lambda expression will map to the signature of an abstract method defined in that functional interface. This effectively means it is a mandate to have an interface (Functional Interface) with only one abstract method (Functional Descriptor) to create a lambda expression.\n//Kotlin\nval isPositive: (Int) -&amp;gt; Boolean = { it &amp;gt; 0 }\nOR,\nval isPositive: (Int) -&amp;gt; Boolean = { num &amp;gt; 0 }\nOR,\nval isPositive: (Int) -&amp;gt; Boolean = { num: Int &amp;gt; 0 }\n\n//Usage\nisPositive(10) returns true\nisPositive(-1) returns false\nAbove, the variable isPositive is a function that takes an Int as an argument and returns a Boolean. The value of this variable is a function definition or a lambda defined in braces, which checks that the passed argument is greater than zero.\nWhereas, as seen in Java below, Predicate is a functional interface containing an abstract method test() — which takes an argument of type T and returns a boolean.\nSo, isPositive takes an argument of type Integer and checks that it is greater than zero. In order to use it, we need to invoke the test() method on isPositive.\n//Java 8\nprivate Predicate&amp;lt;Integer&amp;gt; isPositive = (Integer arg) -&amp;gt; arg &amp;gt; 0;\n\n//Usage\nisPositive.test(10) returns true\nisPositive.test(-1) returns false\n\n@FunctionalInterface\npublic interface Predicate&amp;lt;T&amp;gt; {\n    boolean test(T t);\n}\nLambdas should be independent of functional interfaces and their functional descriptors\nSupport Extension Functions\nKotlin supports extension functions, which provide the ability to extend a class with new functionality without having to inherit from the class or use any type of design pattern, such as Decorator.\nLet&#39;s write an extension function to return the last character of a String, meaning &quot;Kotin&quot;.lastChar() will return &#39;n&#39;.\n//Kotlin\nfun String.lastChar() = this.toCharArray()[this.length - 1]\n\n/**\n    Extension functions are of the form -\n    fun &amp;lt;ReceiverObject&amp;gt;.function_name() = body\n    OR,\n    fun &amp;lt;ReceiverObject&amp;gt;.function_name(arg1: Type1, ... argN: TypeN) = body\n**/\nHere, lastChar() is an extension function defined on String, which is called a receiver object. This function can now be invoked as &quot;Kotlin&quot;.lastChar().\nExtension functions provide an ability to extend a class with new functionalities without inheritance or any other design pattern\nTail Recursion\nKotlin provides support for Tail-recursion. Tail-recursion is a form of recursion in which the recursive calls are the last instructions in the function (tail). In this way, we don&#39;t care about previous values, and one stack frame suffices for all of the recursive calls; tail-recursion is one way of optimizing recursive algorithms.\nThe other advantage/optimization is that there is an easy way to transform a tail-recursive algorithm to an equivalent one that uses iteration instead of recursion.\n//Kotlin\nfun factorialTco(val: Int): Int {\n    tailrec fun factorial(n: Int, acc: Int): Int = if ( n == 1 ) acc else factorial(n - 1, acc * n)\n  return  factorial(val, acc = 1)\n}\nWhen a function is marked with the tailrec modifier and meets the required form, the compiler optimizes out the recursion, leaving behind a fast and efficient loop-based version instead.\nEffectively, a tail-recursive function can execute in constant stack space, so it&#39;s really just another formulation of an iterative process\nJava does not directly support tail-call optimization at the compiler level, but one can use lambda expressions to implement it. It would be nice to see TCO at the compiler level.\nMiscellaneous\n\nRemove inherent duplication [new, return, semicolon]: Kotlin does not require new to create an instance. It still needs a return if a function is treated as a statement instead of an expression.\n\n//Kotlin\nclass Employee(private val id: Int, private val department: Department?) {\n    //no return\n    fun departmentNameWithoutReturn() = department?.name ?: &quot;Unassigned&quot;\n    //return is needed if a function is treated as a statmentrather than an expression\n    fun departmentNameWithoutReturn() {\n        val departmentName = department?.name ?: &quot;Unassigned&quot;\n        return departmentName\n    }\n}\n\nSingleton Classes: It would be great to see an easier way to create singleton classes in Java. An equivalent syntax in Kotlin is seen below.\n\n//Kotlin\nobject DataProviderManager {\n    fun registerDataProvider(provider: DataProvider) {\n        // ...\n    }\n}\n\nImmutable Classes: It would be good to see something like the readonly/immutable modifier to create an immutable class. The below mentioned code snippet is simply a thought (not available in Kotlin or Java).\n\n//Hypothetical [Not available so far]\nimmutable class User(private val name: String, private val id: Int)\nIn conclusion, as developers, we will always make mistakes (skipping NULL checks, mutating a collection, etc.), but providing such features at the language level will make our lives easier and prevent mistakes.\n"
} ,
  
  {
    "title"    : "Let’s deal with Legacy Code",
    "category" : "",
    "tags"     : " Broken Window Theory, Legacy Code, Refactoring",
    "url"      : "/lets-deal-with-legacy-code/",
    "date"     : "April 12, 2018",
    "excerpt"  : "This article is in continuation with the previous article where we defined some of the key aspects of Legacy Code. In this article we will take a Legacy code and add a new feature to it.\nBefore we begin with an example, let’s take a moment to unde...",
  "content"  : "This article is in continuation with the previous article where we defined some of the key aspects of Legacy Code. In this article we will take a Legacy code and add a new feature to it.\nBefore we begin with an example, let’s take a moment to understand Broken Window Theory.\n\nBroken Window Theory\nAn academic theory proposed by James Q. Wilson and George Kelling in 1982 that used broken windows as a metaphor for disorder within neighbourhoods.\nOne broken window, if left unrepaired for a substantial amount of time, instills a sense of abandonment. So another window gets broken. People start littering. Graffiti appears. Serious structural damage begins. In a relatively short time, the building becomes damaged beyond the owner’s desire to fix it, and the sense of abandonment becomes reality.\nLet’s not abandon our code, let’s repair the code as soon as we get an opportunity to repair it and let’s not get ourselves into a situation where damage is beyond our capacity to fix. Let’s see our theory in action.\nProblem Definition Overview\nThe below code belongs to a hypothetical application “Movie Rental” which allows its customers to rent either a Regular or a Children’s movies for fixed number of days. The application also allows generation of a statement which the business calls as “Text Statement”. This application has been running in Production for a long time without issues and has become very popular. Now business wants to generate an HTML statement with exact same logic for amount computation.\npublic class Customer {\n    private String name;\n    private List&amp;lt;Rental&amp;gt; rentals = new ArrayList&amp;lt;&amp;gt;();\n\n    public Customer(String name) {\n        this.name = name;\n    }\n    public void addRental(Rental arg) {\n        rentals.add(arg);\n    }\n    public String getName() {\n        return name;\n    }\n    public String statement() {\n        double totalAmount = 0;\n        String result = &quot;Rental Record for &quot; + getName() + &quot;\\n&quot;;\n        for (Rental each : rentals) {\n            double thisAmount = 0;\n\n            //determine amounts for each line\n            switch (each.getMovie().getPriceCode()) {\n                case Movie.REGULAR:\n                    thisAmount += 2;\n                    if (each.getDaysRented() &amp;lt; 2)\n                        thisAmount += (each.getDaysRented() - 2) * 1.5;\n                    break;\n                case Movie.CHILDRENS:\n                    thisAmount += 1.5;\n                    if (each.getDaysRented() &amp;lt; 3)\n                        thisAmount += (each.getDaysRented() - 3) * 1.5;\n                    break;\n            }\n            //show figures for this Rental\n            result += &quot;\\t&quot; + each.getMovie().getTitle() + &quot;\\t&quot; +\n                    String.valueOf(thisAmount) + &quot;\\n&quot;;\n            totalAmount += thisAmount;\n        }\n        //add footer lines result\n        result += &quot;Amount owed is &quot; + String.valueOf(totalAmount) + &quot;\\n&quot;;\n        return result;\n    }\n}\n\npublic class Movie {\n    public static final int CHILDRENS = 2;\n    public static final int REGULAR = 0;\n\n    private String title;\n    private int priceCode;\n\n    public Movie(String title, int priceCode) {\n        this.title = title;\n        this.priceCode = priceCode;\n    }\n    //getters ignored\n}\n\npublic class Rental {\n    private int daysRented;\n    private Movie movie;\n\n    public Rental(Movie movie, int daysRented){\n        this.movie = movie;\n        this.daysRented = daysRented;\n    }\n    //getters ignored\n}\nThe team decides to discuss different ways to handle this new requirement in legacy code.\n\n    \n\nAnd the team agrees to improve the code before implementing the new functionality. Scott and Jessica will be pairing on this. But, where do they start from? As mentioned in their discussion, they need to understand the code first so they decide to write Characterization Test(s).\nFirst Characterization Test\nScott&amp;gt; How many tests should we write?\nJessica&amp;gt; Let’s look at the code. It should give us some hints.\nScott&amp;gt; I get it. We need a few rentals consisting of Regular and Children’s movie and the number of days rented for which these movies can be greater than 2 or 3. So, one test should cover a decent functionality.\nJessica&amp;gt; I can’t agree more. So let’s write it then.\npublic class CustomerUnitTest {\n    @Test\n    public void shouldGenerateStatement(){\n        Customer john      = new Customer(&quot;John&quot;);\n        Movie    regular   = new Movie(&quot;Black Panther&quot;, REGULAR);\n        Movie    children  = new Movie(&quot;Lion King&quot;,     CHILDRENS);\n        Rental rental1     = new Rental(regular, 3);  \n        Rental rental2     = new Rental(children, 4);\n        john.addRental(rental1);\n        john.addRental(rental2);\n        \n        String statement = john.statement();\n        assertEquals(&quot;&quot;, statement);\n    }\n}\nScott&amp;gt; Let’s run this and see it fail.\norg.junit.ComparisonFailure:\nExpected: &quot;&quot;\nActual:\nRental Record for John\n Black Panther 3.5\n Lion King 3.0\nAmount owed is 6.5\n\nJessica&amp;gt; Great. We have made some progress. Let’s correct our test.\npublic class CustomerUnitTest {\n    @Test\n    public void shouldGenerateStatement(){\n        String expected = &quot;Rental Record for John\\n&quot; +\n                &quot;\\tBlack Panther\\t3.5\\n&quot; +\n                &quot;\\tLion King\\t3.0\\n&quot; +\n                &quot;Amount owed is 6.5\\n&quot;;\n        \n        Customer john      = new Customer(&quot;John&quot;);\n        Movie    regular   = new Movie(&quot;Black Panther&quot;, REGULAR);\n        Movie    children  = new Movie(&quot;Lion King&quot;,     CHILDRENS);\n        Rental rental1     = new Rental(regular, 3);\n        Rental rental2     = new Rental(children, 4);\n        john.addRental(rental1);\n        john.addRental(rental2);\n        \n        String statement = john.statement();\n        assertEquals(expected, statement);\n    }\n}\nJessica and Scott agree to write one test case covering a decent portion of the code. If this gives us confidence, we can live with one test for now else we can write a few more or include movies with daysRented &amp;lt; 2.\nScott&amp;gt; Jessica, what type of test should a Characterization test be? Unit, Functional, Integration?\nJessica&amp;gt; Scott, it is not always possible to write unit or functional tests for legacy code. You might end up writing an integration test to begin with because you just want to know what system does. But, as soon as you get an opportunity, get your tests closer to the code.\nScott&amp;gt; Sure Jessica, let’s start the fun part. Let’s fix a broken window.\nRefactoring\nScott&amp;gt; Where do we start from?\nJessica&amp;gt; I believe statement() method is a long method. We should try and make it a little shorter.\nScott&amp;gt; Agreed.\nJessica and Scott agreed that statement() method is a long method. But, this agreement was not based on the number of lines in the method. It was based on how easy it is to comprehend the method or is a method doing more than one thing at a time or it can be decomposed further.\npublic String statement() {\n    double totalAmount = 0;\n    String result = &quot;Rental Record for &quot; + getName() + &quot;\\n&quot;;\n    for (Rental each : Rentals) {\n        //determine amounts for each line\n        double thisAmount = amount(each);\n\n        //show figures for this Rental\n        result += &quot;\\t&quot; + each.getMovie().getTitle() + &quot;\\t&quot; + String.valueOf(thisAmount) + &quot;\\n&quot;;\n        totalAmount += thisAmount;\n    }\n    //add footer lines result\n    result += &quot;Amount owed is &quot; + String.valueOf(totalAmount) + &quot;\\n&quot;;\n    return result;\n}\n\nprivate double amount(Rental each) {\n    double thisAmount = 0.0;\n    switch (each.getMovie().getPriceCode()) {\n        case Movie.REGULAR:\n            thisAmount += 2;\n            if (each.getDaysRented() &amp;lt; 2)\n                thisAmount += (each.getDaysRented() - 2) * 1.5;\n            break;\n        case Movie.CHILDRENS:\n            thisAmount += 1.5;\n            if (each.getDaysRented() &amp;lt; 3)\n                thisAmount += (each.getDaysRented() - 3) * 1.5;\n            break;\n    }\n    return thisAmount;\n}\nJessica&amp;gt; Switch statement has gone out and the extracted amount() method does one thing which is getting amount for a given rental.\nScott&amp;gt; Let’s continue refactoring. I am in a mood to clean up everything.\nJessica&amp;gt; Hold on Scott, we need to run tests before we move on.\nAnd the test ran successfully.\nWhile working with Legacy Code it is important to take smaller steps and follow refactoring cycle. Refactor -&amp;gt; Run Tests -&amp;gt; Refactor\nScott&amp;gt; Sure. Jessica, are we in a position to remove the comment “determine amounts for each line” from previous code?\nJessica&amp;gt; Yes, we can remove it.\npublic String statement() {\n    double totalAmount = 0;\n    String result = &quot;Rental Record for &quot; + getName() + &quot;\\n&quot;;\n    for (Rental rental : Rentals) {\n        double thisAmount = amount(rental);\n\n        //show figures for this Rental\n        result += &quot;\\t&quot; + rental.getMovie().getTitle() + &quot;\\t&quot; + String.valueOf(thisAmount) + &quot;\\n&quot;;\n        totalAmount += thisAmount;\n    }\n    //add footer lines result\n    result += &quot;Amount owed is &quot; + String.valueOf(totalAmount) + &quot;\\n&quot;;\n    return result;\n}\n\nprivate double amount(Rental rental) {\n    double thisAmount = 0.0;\n    switch (rental.getMovie().getPriceCode()) {\n        case Movie.REGULAR:\n            thisAmount += 2;\n            if (rental.getDaysRented() &amp;lt; 2)\n                thisAmount += (rental.getDaysRented() - 2) * 1.5;\n            break;\n        case Movie.CHILDRENS:\n            thisAmount += 1.5;\n            if (rental.getDaysRented() &amp;lt; 3)\n                thisAmount += (rental.getDaysRented() - 3) * 1.5;\n            break;\n    }\n    return thisAmount;\n}\nRemove comments from legacy code when you have captured their complete essence . Though I did take some liberty to rename variable along with removing comment, it is always ideal to take smaller steps when you are beginning to understand legacy code. As you grow in confidence, you might want to take bigger steps but one test failure and the reality reveals itself.\nScott&amp;gt; Let’s look at amount() method. It depends on priceCode from movie but is placed in Customer. We should move this method to the place where it belongs.\nJessica&amp;gt; Yes, let’s do a few method movements (in the interest of this article).\n//Customer\npublic String statement() {\n    double totalAmount = 0;\n    String result = &quot;Rental Record for &quot; + getName() + &quot;\\n&quot;;\n    for (Rental rental : Rentals) {\n        double thisAmount = rental.amount();\n\n        //show figures for this Rental\n        result += &quot;\\t&quot; + rental.movieTitle() + &quot;\\t&quot; + String.valueOf(thisAmount) + &quot;\\n&quot;;\n        totalAmount += thisAmount;\n    }\n    //add footer lines result\n    result += &quot;Amount owed is &quot; + String.valueOf(totalAmount) + &quot;\\n&quot;;\n    return result;\n}\n\n//Rental\ndouble amount() {\n  return movie.amount(this.daysRented);\n}\n\n//Movie\ndouble amount(int daysRented) {\n    double thisAmount = 0.0;\n    switch (this.getPriceCode()) {\n        case Movie.REGULAR:\n            thisAmount += 2;\n            if (daysRented &amp;lt; 2)\n                thisAmount += (daysRented - 2) * 1.5;\n            break;\n        case Movie.CHILDRENS:\n            thisAmount += 1.5;\n            if (daysRented &amp;lt; 3)\n                thisAmount += (daysRented - 3) * 1.5;\n            break;\n    }\n    return thisAmount;\n}\nI did a few movements. Moved amount() method to Rental and then to Movie and ran the tests. It should be noted that this is our first opportunity to write unit tests for Rental and Movie. I won’t, for this article, but I assume you will.\nScott&amp;gt; Jessica, I have a question. Movie has a switch statement based on different types of movies. Shall we introduce some polymorphism here?\nJessica&amp;gt; I don’t think it is coming in our way of implementing HTML statement functionality.\nScott has raised a valid point but we need to remember one thing, “we refactor the code which comes in our way”. At this point in time, we need to implement HTML statement and switch code does not come in the way of our new feature, neither do the magic numbers 2 or 1.5. If you want to continue with small refactorings which are not coming in your way, say changing Magic Numbers to Constants, go ahead and do it but do not move away from your actual task: implementing HTML statement.\nScott&amp;gt; I get that. Thank you. The statement() method in Customer is short enough. Shall we pause our refactoring here?\nJessica&amp;gt; We could, but one thing that is bothering me is this method seems to be generating 3 parts of the statement and I can see it clearly — header, body and footer. If the effort is not huge we should try and extract this code into different methods.\nScott&amp;gt; You clearly have an eye for refactoring. Let’s do it.\npublic String textStatement() {\n    return textHeader() + textBody() + textFooter();\n}\nprivate String textHeader() {\n    return &quot;Rental Record for &quot; + getName() + &quot;\\n&quot;;\n}\nprivate String textBody() {\n    String result = &quot;&quot;;\n    for (Rental rental : Rentals) {\n        result += &quot;\\t&quot; + rental.movieTitle() + &quot;\\t&quot; + String.valueOf(rental.amount()) + &quot;\\n&quot;;\n    }\n    return result;\n}\nprivate String textFooter() {\n    return &quot;Amount owed is &quot; + String.valueOf(totalAmount()) + &quot;\\n&quot;;\n}\nprivate double totalAmount() {\n    double totalAmount = 0.0;\n    for (Rental rental : Rentals) {\n        totalAmount += rental.amount();\n    }\n    return totalAmount;\n}\nI cheated again. Did lot more than what I should have done,renamed methods to be text*, duplicated for loops (over rentals) to calculate totalAmount(), repeated the same in textBody().\nIs that justified? Well, how many rentals do we expect to have for a customer? What is the cost of iterating over them twice? If it is not significant, go ahead and use it. What does it give me? Look at the statement() (renamed as textStatement()) method now.\nJessica&amp;gt; Now, we are done with refactoring. We can introduce HTML statement functionality now.\nConclusion\nJessica and Scott went on to implement HTML functionality (with tests) and they did a lot to clean up the existing code. This is much more understandable that it used to be.\nThey might not have cleaned up everything but they clearly have left a great deal of understanding trace for others to follow.\nThey followed Cover and Modify, Boy Scout rule, Refactoring Cycle and refactored enough to finish the new functionality, in-short dealt with Legacy Code professionally.\nReferences\n\nRefactoring — Improving The Design Of Existing Code\nRefactoring Catalog\n"
} ,
  
  {
    "title"    : "Let’s define Legacy Code",
    "category" : "",
    "tags"     : " Boy Scout Rule, Legacy Code",
    "url"      : "/lets-define-legacy-code/",
    "date"     : "April 10, 2018",
    "excerpt"  : "“I have been having sleepless nights trying to add features in the code we acquired from other company. I am dealing with purest form of Legacy Code”\n“I am having a real hard time dealing with tangled, unstructured code that I have to work with bu...",
  "content"  : "“I have been having sleepless nights trying to add features in the code we acquired from other company. I am dealing with purest form of Legacy Code”\n“I am having a real hard time dealing with tangled, unstructured code that I have to work with but I don’t understand a bit. Legacy Code !”\nLegacy code is a term which probably has a lot of different definitions like -code acquired from someone else, code written by someone else, code that is hard to understand or code written in outdated technologies. Whatever be the definition, most of us believe Legacy Code is Scary.\nQuestion&amp;gt; How would you define legacy code?\nDefining Legacy Code\nMichael Feathers in his book “Working Effectively with Legacy Code” defines legacy code as, code without tests.\nCode without tests is a bad code. It doesn’t matter how well written it is; how well structured it is; how well encapsulated it is.Without tests there is no way to tell if our code is getting better or worse.\nWell, a slightly modified version of this definition is “code without unit tests is called legacy code”. It is always better to have tests as close to the code as possible (unit tests &amp;gt; integration tests &amp;gt; UI tests). So, it would not be unfair to call a code without unit tests a legacy code.\nWorking with Legacy Code\nQuestion&amp;gt; What approach will you take if you were to make a change in legacy code?\nMost of us might say, “I will make the change and call it a day, why bother about improving the code”. Rationale behind this thought process could be -\n\nI don’t have enough time to refactor the code, I would prefer making a change and completing my story\nWhy risk changing the structure of the code that has been running in production for a long time\nWhat is the overall benefit of refactoring legacy code\n\nMichael Feathers calls this style of making a change as Edit and Pray. You plan and make your changes and when you are done, you pray and pray harder to get your changes right.\nWith this style, one can only contribute to increasing Legacy code.\n\nThere is a different style of making changes which is Cover and Modify. Build a Safety Net, make changes in the system, let Safety Net provide feedback and work on those feedbacks.\nIt can be safely assumed that Cover and Modify is a way to go to deal with Legacy code.\nQuestion&amp;gt; But, should you even spend time writing tests in legacy code or even thinking about refactoring a legacy code?\nThe Boy Scout Rule\nThe idea behind the Boy Scout Rule, as stated by Uncle Bob, is fairly simple: Leave the code cleaner than you found it! Whenever you touch an old code, you should clean it properly. Do not just apply a shortcut solution that will make the code more difficult to understand but instead treat it with care. It’s not enough to write code well, the code has to be kept clean over time.\nWe get a very strong message when Boy Scout rule is applied to legacy code “leave a trace of understanding behind you for others to follow”, which means we will refactor the code to make it more understandable. And in order to refactor, we will build Safety Net around it.\nNow that we understand we can not take shortcuts the only option that is left with us is to write some tests, refactor code and proceed with the development.\nQuestions&amp;gt;\n\nWhich tests should we write?\nHow much should we refactor?\n\nWhich Tests To Write\nIn nearly every legacy system, what the system does is more important than what it is supposed to do.\nCharacterization Tests, the tests that we need when we want to preserve behavior are called as characterization tests. A characterization test is a test that characterizes the actual behavior of a piece of code. There’s no “Well, it should do this” or “I think it does that”. The tests document the actual current behavior of the system.\nWriting Characterization Test\nA Characterization Test by definition documents the actual current behavior of the system the exact same way it is running on Production environment.\nLet’s write a Characterization test for a Customer object which generates text statement for some movies rented by a customer.\nimport static com.code.legacy.movie.MovieType.CHILDREN;\nimport static org.junit.Assert.assertEquals;\n\npublic void shouldGenerateTextStatement(){\n   Customer john          = new Customer(&quot;John&quot;);\n   Movie    childrenMovie = new Movie(&quot;Toy Story&quot;, CHILDREN);   \n   int      daysRented    = 3;\n   Rental   rental        = new Rental(childrenMovie, daysRented);\n   john.addRental(rental);\n   \n   String statement = john.generateTextStatement();\n   assertEquals(&quot;&quot;, statement);\n}\nThis test attempts to understand (or characterize) the “Text Statement” generation for a customer given a children’s movie rented for 3 days. Because we do not understand the system (at least as of now), we expect the statement to be blank or containing any dummy value.\nLet’s run the test and let it fail. When it does, we have found out what the code actually does under that condition.\njava.lang.AssertionError:\nExpected :&quot;&quot;\nActual   :Rental Record for John, Total amount owed = 12.5. You earned 4 frequent renter points.\nNow, that we know the behavior of the code, we can go ahead and change the test.\nimport static com.code.legacy.movie.MovieType.CHILDREN;\nimport static org.junit.Assert.assertEquals;\n\npublic void shouldGenerateTextStatement(){\n   String expectedStatement = &quot;Rental Record for John, Total amount  owed = 12.5. You earned 4 frequent renter points&quot;;\n   Customer john          = new Customer(&quot;John&quot;);\n   Movie    childrenMovie = new Movie(&quot;Toy Story&quot;, CHILDREN);   \n   int      daysRented    = 3;\n   Rental   rental        = new Rental(childrenMovie, daysRented);\n   john.addRental(rental);\n   \n   Sting statement = john.generateTextStatement();\n   assertEquals(expectedStatement, statement);\n}\nHold on, did we just copy the output generated by the code and placed into our test. Yes, that is exactly what we did.\nWe aren’t trying to find bugs right now. We are trying to put in a mechanism to find bugs later, bugs that show up as differences from the system’s current behavior. When we adopt this perspective, our view of tests is different: They don’t have any moral authority; they just sit there documenting what system really does. At this stage, it’s very important to have that knowledge of what the system actually does someplace.\nQuestion&amp;gt; What is the total number of tests that we write to characterize a system?\nAnswer&amp;gt; It’s infinite. We could dedicate a good portion of our lives to writing case after case for any class in a legacy code.\nQuestion&amp;gt; When do we stop then? Is there any way of knowing which cases are more important than others?\nAnswer&amp;gt; Look at the code we are characterizing. The code itself can give us ideas about what it does, and if we have questions, tests are an ideal way of asking them. At that point, write a test or tests that cover good enough portion of the code.\nQuestion&amp;gt; Does that cover everything in the code?\nAnswer&amp;gt; It might not. But then we do the next step. We think about the changes that we want to make in the code and try to figure out whether the tests that we have will sense any problems that we can cause. If they won’t, we add more tests until we feel confident that they will.\nHow Much To Refactor?\nThere is so much to refactor in legacy code and we can not refactor everything. In order to answer this we need to go back to understanding our purpose of refactoring the legacy code.\nWe want to refactor legacy code to leave it cleaner than what it was when it came to us and to make it understandable for others.\nWith that said, we want to make the system better keeping the focus on the task. We don’t want go crazy with refactoring trying to rewrite the whole system in a few days. What we want to do is “refactor the code that comes in our way of implementing any new change”. We will try and understand this better with an example in the next article.\n\n   \n\n\nReferences\n\nWorking Effectively with Legacy Code\n"
} ,
  
  {
    "title"    : "Flips: Feature Flipping for Java",
    "category" : "",
    "tags"     : " Feature Toggles, Flips, Spring Boot, Spring MVC",
    "url"      : "/flips-feature-flipping-for-java/",
    "date"     : "October 7, 2017",
    "excerpt"  : "\nFlips is an implementation of the Feature Toggles pattern for Java and Spring (Spring Core / Spring MVC/ Spring Boot) based application.\nFeature Toggle is a powerful technique that allows teams to modify system behavior and deliver new functional...",
  "content"  : "\nFlips is an implementation of the Feature Toggles pattern for Java and Spring (Spring Core / Spring MVC/ Spring Boot) based application.\nFeature Toggle is a powerful technique that allows teams to modify system behavior and deliver new functionality to users rapidly but safely.\nWhy Another Library for Feature Toggles?\nThe idea behind Flips is to let the clients implement toggles with minimum configuration and coding.\nThe main motivations behind implementing this library were -\n\nShould be simple to use\nShould require minimal configuration and code\nShould be able to flip a feature based on various conditions\nShould be able to flip a feature based on a combination of different conditions\nShould be possible for the clients to create custom conditions to suit their requirements\n\nFlips works with Java 8 and Spring Core/Spring MVC/Spring Boot, and is available for web and non-web applications.\nWhat Does Flips Offer?\nFlips provides various conditions to flip a feature. The image below summarizes the features:\n\nAny feature can be flipped ON or OFF based on different conditions which can be value of a property, current active profiles, days of the week, or a combination of these, etc.\nLet’s get started with in-depth understanding of these features.\nGetting Started\nInclude the necessary dependency:\n&amp;lt;dependency&amp;gt;\n   &amp;lt;groupId&amp;gt;com.github.feature-flip&amp;lt;/groupId&amp;gt;\n   &amp;lt;artifactId&amp;gt;flips-web&amp;lt;/artifactId&amp;gt;\n   &amp;lt;version&amp;gt;1.0.1&amp;lt;/version&amp;gt;\n&amp;lt;/dependency&amp;gt;\nOr:\n&amp;lt;dependency&amp;gt;\n  &amp;lt;groupId&amp;gt;com.github.feature-flip&amp;lt;/groupId&amp;gt;\n  &amp;lt;artifactId&amp;gt;flips-core&amp;lt;/artifactId&amp;gt;\n  &amp;lt;version&amp;gt;1.0.1&amp;lt;/version&amp;gt;\n&amp;lt;/dependency&amp;gt;\nDetailed Description of All Annotations\nFlips provides various annotations to flip a feature. Let’s have a detailed walk-through of all the annotations:\n@FlipOnEnvironmentProperty\n@FlipOnEnvironmentProperty is used to flip a feature based on the value of an environment property.\n@Component\nclass EmailSender {\n    @FlipOnEnvironmentProperty(property = &quot;feature.send.email&quot;, \n                               expectedValue = &quot;true&quot;)\n    public void sendEmail(EmailMessage emailMessage) {\n    }\n}\n@FlipOnProfiles\n@FlipOnProfiles is used to flip a feature based on the environment in which the application is running\n@Component\nclass EmailSender {\n    @FlipOnProfiles(activeProfiles = {&quot;dev&quot;, &quot;qa&quot;})\n    public void sendEmail(EmailMessage emailMessage) {\n    }\n}\nFeature sendEmail is enabled if the current profile (or environment) is either dev or qa.\n@FlipOnDaysOfWeek\n@FlipOnDaysOfWeek is used to flip a feature based on the day of the week.\n@Component\nclass EmailSender {\n    @FlipOnDaysOfWeek(daysOfWeek = {DayOfWeek.MONDAY})\n    public void sendEmail(EmailMessage emailMessage) {\n    }\n}\nFeature sendEmail is enabled if the current day is MONDAY.\n@FlipOnDateTime\n@FlipOnDateTime is used to flip a feature based on date and time.\n@Component\nclass EmailSender {\n    @FlipOnDateTime(cutoffDateTimeProperty = &quot;default.date.enabled&quot;)\n    public void sendEmail(EmailMessage emailMessage) {\n    }\n}\nFeature sendEmail is enabled if the current datetime is equal to or greater than the value (in ISO-8601 format) defined by the default.date.enabled property.\n@FlipOnSpringExpression\n@FlipOnSpringExpression is used to flip a feature based on the evaluation of a Spring expression.\n@Component\nclass EmailSender {\n    @FlipOnSpringExpression(expression = &quot;T(java.lang.Math).sqrt(4) * 100.0 \n                                          &amp;lt; T(java.lang.Math).sqrt(4) * 10.0&quot;)\n    public void sendEmail(EmailMessage emailMessage) {\n    }\n}\nFeature sendEmail is enabled if the expression evaluates to TRUE. This annotation happens to be one of the most powerful annotations in Flips library. Why so ?\nOne could always write a custom spring component and use the same in @FlipOnSpringExpression to flip a feature.\n@FlipBean\n@FlipBean is used to flip the invocation of a method with another method defined in a different bean.\n@Component\nclass EmailSender {\n    @FlipBean(with = SendGridEmailSender.class)\n    @FlipOnProfiles(activeProfiles = &quot;DEV&quot;)\n    public void sendEmail(EmailMessage emailMessage) {\n    }\n}\nThis will flip the invocation of the sendEmail method with a method (having same signature) defined in SendGridEmailSender Spring component if the current profile is DEV.\n@FlipOff\n@FlipOff is used to flip a feature off.\n@Component\nclass EmailSender {\n    @FlipOff\n    public void sendEmail(EmailMessage emailMessage) {\n    }\n}\nFeature sendEmail is always DISABLED.\nCombining annotations\n@Component\nclass EmailSender {\n    @FlipOnProfiles(activeProfiles = &quot;dev&quot;)\n    @FlipOnDaysOfWeek(daysOfWeek={DayOfWeek.MONDAY})\n    public void sendEmail(EmailMessage emailMessage) {\n    }\n}\nFeature sendEmail is enabled if the current profile is &quot;dev&quot; AND the current day of the week is MONDAY.\nImport Flip Context Configuration\nIn order to bring all Flips-related annotations into effect, you need to import FlipContextConfiguration or FlipWebContextConfiguration and you are ready to go.\n@SpringBootApplication\n@Import(FlipWebContextConfiguration.class)\nclass ApplicationConfig {\n    public static void main(String[] args) {\n        SpringApplication.run(ApplicationConfig.class, args);\n    }\n}\nPlease refer to this sample project.\nCreating Custom Annotations\nAll the annotations provided by the library are of type @FlipOnOff, which is essentially a meta-annotation. So, create a custom annotation annotated with @FlipOnOff at the method level:\n@Target({ElementType.METHOD})\n@Retention(RetentionPolicy.RUNTIME) \n@FlipOnOff(value = MyCustomCondition.class) !!Important\npublic @interface MyCustomAnnotation {\n}\nAs a part of this annotation, specify the condition that will evaluate this annotation.\n@Component\npublic class MyCustomCondition implements FlipCondition {\n    @Override\n    public boolean evaluateCondition(FeatureContext fContext,\n                                     FlipAnnotationAttributes attr){\n        \n        //code to evaluate flip condition\n        return false;\n    }\n}\nThis Condition class needs to implement FlipCondition and MUST be a Spring Component.\nThat is it! You can use your custom annotation to any method to flip it ON or OFF based on your condition.\nWhat Does It Mean “Feature Is DISABLED”?\nFeatureNotEnabledException is thrown if a disabled feature is invoked. In case of a web application, one could use flips-web dependency, which also provides a ControllerAdvice meant to handle this exception.\nIt returns a default response and a status code of 501, which can be overridden. Please refer to the sample project for more information.\nWrap Up\nWe believe the MVP is done and features like flipping at runtime and supporting database-driven feature flips are in the pipeline.\nFor any custom flip condition, one could go ahead and use @FlipOnSpringExpression with your custom spring bean to determine flip condition.\nIf you want to have a look at the code or even want to contribute, you can check out Flips. Feel free to give any feedback.\n"
} 
  
  
  
]
